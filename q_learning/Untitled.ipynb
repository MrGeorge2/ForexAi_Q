{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, CuDNNLSTM, Conv1D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import time as t_lib\n",
    "import tensorflow as tf\n",
    "from threading import Thread\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "DATAFRAME_NAME = 'EURUSD_m15_Ask_ready.csv'\n",
    "NUMBER_OF_SAMPLES = 20\n",
    "\n",
    "EPISODES = 5000\n",
    "TICQTY_MAX = 55000\n",
    "HOLD_REWARD = -50\n",
    "OPEN_TRADE_REWARD = 0\n",
    "CLOSING_TRADE_WITH_OPENING = 100  #20\n",
    "DIVIDE_PRICE_UNDER_LOCAL_MINIMA = 1  #10\n",
    "REWARD_FOR_PIPS = 10000\n",
    "TIMES_FACTOR = 6 #10\n",
    "NEGATIVE_REWARD_DIVIDE = 3 # kolikrat bude mensi times factor pri zaporne odmene\n",
    "\n",
    "ACTION_DECODE = {\n",
    "    0: 0,\n",
    "    1: 0.5,\n",
    "    2: 1,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Dataframe:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._dataframe = self._load()[0:2000]\n",
    "        self.__scaler = MinMaxScaler()\n",
    "\n",
    "    @property\n",
    "    def lenght(self):\n",
    "        return len(self._dataframe.index) - NUMBER_OF_SAMPLES\n",
    "\n",
    "    def get(self, sample_number):\n",
    "        if sample_number > self.lenght or sample_number < 0:\n",
    "            raise ValueError(\"Sample number out of range (0 - {self.lenght})\")\n",
    "\n",
    "        start_index = sample_number\n",
    "        end_index = start_index + NUMBER_OF_SAMPLES\n",
    "\n",
    "        df_sample = self._dataframe[start_index: end_index]\n",
    "\n",
    "        last_open = df_sample.at[df_sample.index[-1], 'open']\n",
    "        last_close = df_sample.at[df_sample.index[-1], 'close']\n",
    "\n",
    "        df_sample = df_sample[['open', 'close', 'high', 'low', 'tickqty', 'hours', 'minutes']].values\n",
    "        df_sample = self._scale(df_sample, start=0, end=4)\n",
    "        return np.expand_dims(df_sample, axis=0), last_open, last_close\n",
    "\n",
    "    @staticmethod\n",
    "    def _load():\n",
    "        \"\"\" Creating relative path and then loading the df_path \"\"\"\n",
    "        \"\"\"\n",
    "        df_path = os.path.join(os.path.dirname(os.path.abspath(__file__)) +\n",
    "                               os.path.normpath(f'/dfs/{cfg.DATAFRAME_NAME}'))\n",
    "        \"\"\"\n",
    "        df_path = './dfs/{}'.format(DATAFRAME_NAME)\n",
    "        df = pd.read_csv(\n",
    "            df_path,\n",
    "            dtype={\n",
    "                'datetime'\n",
    "                'open': np.float32,\n",
    "                'close': np.float32,\n",
    "                'high': np.float32,\n",
    "                'low': np.float32,\n",
    "                'tickqty': np.float32,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # df['hours']= pd.to_datetime(df['datetime'], format='%Y%m%d %H:%M:%S.%f').dt.hour / 24\n",
    "        df['hours'] = pd.to_datetime(df['date'], format='%m-%d-%Y %H:%M:%S').dt.hour / 24\n",
    "        df['minutes'] = pd.to_datetime(df['date'], format='%m-%d-%Y %H:%M:%S').dt.minute / 64\n",
    "        df['tickqty'] = df['tickqty'] / TICQTY_MAX\n",
    "        return df\n",
    "\n",
    "    def _scale(self, array: np.ndarray, start: int, end: int):\n",
    "        columns = array.T[start: end].T\n",
    "\n",
    "        self.__scaler.fit(columns)\n",
    "        scaled_cols = self.__scaler.transform(columns).T\n",
    "        array.T[start:end] = scaled_cols\n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, batch_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=20000)\n",
    "        self.sample_memory = deque()\n",
    "        \n",
    "        self.gamma = 0.99  # discount rate\n",
    "        \n",
    "        self.epsilon = 1 #0.44 # exploration rate\n",
    "        self.epsilon_min = 0.0\n",
    "        self.epsilon_decay = 1 #0.999\n",
    "        \n",
    "        self.learning_rate = 0.0009\n",
    "        self.learning_rate_decay = 0.9999\n",
    "        self.learning_rate_min = 0.0001\n",
    "        \n",
    "        self.batch_size_samples = 500\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    \"\"\"Huber loss for Q Learning\n",
    "\n",
    "    References: https://en.wikipedia.org/wiki/Huber_loss\n",
    "                https://www.tensorflow.org/api_docs/python/tf/losses/huber_loss\n",
    "    \"\"\"\n",
    "\n",
    "    def _huber_loss(self, y_true, y_pred, clip_delta=1.0):\n",
    "        error = y_true - y_pred\n",
    "        cond = K.abs(error) <= clip_delta\n",
    "\n",
    "        squared_loss = 0.5 * K.square(error)\n",
    "        quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
    "\n",
    "        return K.mean(tf.where(cond, squared_loss, quadratic_loss))\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Conv1D(filters=32,\n",
    "                         kernel_size=12,\n",
    "                         activation=\"relu\",\n",
    "                         padding=\"same\",\n",
    "                         input_shape=self.state_size))\n",
    "                  \n",
    "        model.add(Conv1D(filters=24,\n",
    "                         kernel_size=8,\n",
    "                         activation=\"relu\",\n",
    "                         padding=\"same\"))\n",
    "                  \n",
    "                  \n",
    "        model.add(Flatten())          \n",
    "        model.add(Dense(units=32, init=\"uniform\", activation=\"relu\"))\n",
    "        model.add(Dense(units=16, init=\"uniform\", activation=\"relu\"))\n",
    "                  \n",
    "        \"\"\"\n",
    "        \n",
    "        model.add(CuDNNLSTM(units=171, return_sequences=True, input_shape=self.state_size))\n",
    "        model.add(CuDNNLSTM(units=114, return_sequences=True)) \n",
    "        model.add(CuDNNLSTM(units=76, return_sequences=True)) \n",
    "        model.add(CuDNNLSTM(units=50, return_sequences=True)) \n",
    "        model.add(CuDNNLSTM(units=33, return_sequences=True))  \n",
    "        model.add(CuDNNLSTM(units=22, return_sequences=True))  \n",
    "        model.add(CuDNNLSTM(units=15, return_sequences=True))\n",
    "        model.add(CuDNNLSTM(units=10, return_sequences=True))\n",
    "        model.add(CuDNNLSTM(units=7, return_sequences=True))\n",
    "        model.add(CuDNNLSTM(units=5, return_sequences=True))\n",
    "        model.add(CuDNNLSTM(units=3, return_sequences=False))\n",
    "        \"\"\"\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss=self._huber_loss,\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        self.sample_memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train_from_iterations(self):\n",
    "        while True:\n",
    "            if len(self.sample_memory) != 0:\n",
    "                state, action, reward, next_state, done = self.sample_memory.popleft()\n",
    "                self.train(state, action, reward, next_state, done)\n",
    "\n",
    "    def train(self, state, action, reward, next_state, done):\n",
    "        target = self.model.predict(state, steps=1, verbose=0)\n",
    "        if done and reward > 80 * TIMES_FACTOR:\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            # a = self.model.predict(next_state)[0]\n",
    "            t = self.target_model.predict(next_state)[0]\n",
    "            target[0][action] = reward + self.gamma * np.amax(t)\n",
    "            # target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
    "        self.model.fit(state, target, epochs=1, verbose=0)\n",
    "        # print('done')\n",
    "\n",
    "    def act(self, state):\n",
    "        if not isinstance(state, np.ndarray):\n",
    "            return 0\n",
    "\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size), True\n",
    "        act_values = self.model.predict(state, steps=1)\n",
    "        return np.argmax(act_values[0]), False  # returns action\n",
    "\n",
    "    def predict(self, state):\n",
    "        act_values = self.model.predict(state, steps=1)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self):\n",
    "        while True:\n",
    "            minibatch = random.sample(self.memory, self.batch_size)\n",
    "            for state, action, reward, next_state, done in minibatch:\n",
    "                if not isinstance(state, np.ndarray):\n",
    "                    continue\n",
    "                \n",
    "                self.train(state, action, reward, next_state, done)\n",
    "                \"\"\"\n",
    "                target = self.model.predict(state, steps=1, verbose=0)\n",
    "                if done and reward > 80 * TIMES_FACTOR:\n",
    "                    target[0][action] = reward\n",
    "                else:\n",
    "                    # a = self.model.predict(next_state)[0]\n",
    "                    t = self.target_model.predict(next_state)[0]\n",
    "                    target[0][action] = reward + self.gamma * np.amax(t)\n",
    "                    # target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
    "                self.model.fit(state, target, epochs=1, verbose=0)\n",
    "                \"\"\"\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "            else:\n",
    "                self.epsilon = self.epsilon_min\n",
    "            \n",
    "            if self.learning_rate > self.learning_rate_min:\n",
    "                self.learning_rate *= self.learning_rate_decay\n",
    "            else:\n",
    "                self.learning_rate = self.learning_rate_min\n",
    "            # print('done')\n",
    "    \n",
    "    def set_learning_rate(self):\n",
    "        K.set_value(self.model.optimizer.lr, self.learning_rate)  # set new lr\n",
    "        K.set_value(self.target_model.optimizer.lr, self.learning_rate)  # set new lr\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "        self.model._make_predict_function()\n",
    "        self.model._make_test_function()\n",
    "        self.model._make_train_function()\n",
    "\n",
    "        self.target_model.load_weights(name)\n",
    "        self.target_model._make_predict_function()\n",
    "        self.target_model._make_test_function()\n",
    "        self.target_model._make_train_function()\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class Trevor:\n",
    "    POSITIVE_TIMES_REWARD = 0 #0.00001\n",
    "    NEGATIVE_TIMES_REWARD = 0 #0.00001\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "        self.cursor = 0\n",
    "        self.enter_price = 0\n",
    "        self.local_max_price = 0\n",
    "\n",
    "        self.last_action = 0\n",
    "\n",
    "        self.closed_counter = 0\n",
    "        self.total_reward = 0\n",
    "        self.trade_counter = 0\n",
    "\n",
    "        self.closed_counter_list = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.cursor = 0\n",
    "        self.enter_price = 0\n",
    "        self.last_action = 0\n",
    "        self.closed_counter = 0\n",
    "        self.trade_counter = 0\n",
    "        self.total_reward = 0\n",
    "        # self.reset_closed_list()\n",
    "\n",
    "        return self.step(0)[0]\n",
    "\n",
    "    def step(self, action):\n",
    "        sample, last_open, last_close = self.df.get(self.cursor)\n",
    "\n",
    "        reward, closing_trade = self.__process_action(action=action, last_close=last_close)\n",
    "        sample = self.__append_last_action(sample=sample, action=action, last_close=last_close)\n",
    "\n",
    "        self.__increment_cursor()\n",
    "\n",
    "        return sample, reward, closing_trade, ''\n",
    "\n",
    "    def get_total_reward(self):\n",
    "        return self.total_reward\n",
    "\n",
    "    def reset_closed_list(self):\n",
    "        self.closed_counter_list = []\n",
    "\n",
    "    def plot(self, title):\n",
    "        x = list(range(1, len(self.closed_counter_list) + 1))\n",
    "        pyplot.plot(x, self.closed_counter_list)\n",
    "        pyplot.title(str(title))\n",
    "        pyplot.show()\n",
    "\n",
    "    def __process_action(self, action, last_close):\n",
    "        if action < 0 or action > 2:\n",
    "            raise ValueError('Action have to be inrage (0 - 2) got {action}')\n",
    "\n",
    "        closing_trade = False\n",
    "\n",
    "        # \"\"\" CLOSING POSITION \"\"\"\n",
    "        if (self.last_action == 2 and action == 0) or (self.last_action == 1 and action == 0):\n",
    "            reward = self.__close_trade(last_close=last_close)\n",
    "            closing_trade = True\n",
    "\n",
    "        # \"\"\" CLOSING POSITION AND GOING TO DIFFERENT POSITION \"\"\"\n",
    "        elif (self.last_action == 2 and action == 1) or (self.last_action == 1 and action == 2):\n",
    "            reward = self.__close_trade(last_close=last_close) - CLOSING_TRADE_WITH_OPENING\n",
    "            self.enter_price = last_close\n",
    "            self.local_max_price = last_close\n",
    "            closing_trade = True\n",
    "\n",
    "        # \"\"\" HOLDING OPENED POSITION  \"\"\"\n",
    "        elif (self.last_action == 2 and action == 2) or (self.last_action == 1 and action == 1):\n",
    "            if self.last_action == 2:\n",
    "                if self.local_max_price < last_close:\n",
    "                    reward = (last_close - self.enter_price) * REWARD_FOR_PIPS\n",
    "                    self.local_max_price = last_close\n",
    "\n",
    "                else:\n",
    "                    reward = (last_close - self.local_max_price) * REWARD_FOR_PIPS\n",
    "                    reward = reward / DIVIDE_PRICE_UNDER_LOCAL_MINIMA if last_close > self.enter_price \\\n",
    "                        else reward\n",
    "\n",
    "            else:\n",
    "                if self.local_max_price > last_close:\n",
    "                    reward = (self.enter_price - last_close) * REWARD_FOR_PIPS\n",
    "                    self.local_max_price = last_close\n",
    "\n",
    "                else:\n",
    "                    reward = (self.local_max_price - last_close) * REWARD_FOR_PIPS\n",
    "                    reward = reward / DIVIDE_PRICE_UNDER_LOCAL_MINIMA if last_close < self.enter_price \\\n",
    "                        else reward\n",
    "\n",
    "        # \"\"\" OPENING POSITION  \"\"\"\n",
    "        elif (self.last_action == 0 and action == 1) or (self.last_action == 0 and action == 2):\n",
    "            self.enter_price = last_close\n",
    "            self.local_max_price = last_close\n",
    "            reward = OPEN_TRADE_REWARD\n",
    "\n",
    "        # \"\"\" HOLD \"\"\"\n",
    "        elif self.last_action == 0 and action == 0:\n",
    "            reward = HOLD_REWARD\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Last action = {self.last_action} and actual_action = {action}')\n",
    "\n",
    "        self.last_action = action\n",
    "        self.total_reward += reward\n",
    "        return reward, closing_trade\n",
    "\n",
    "    def __increment_cursor(self):\n",
    "        \"\"\" Incrementing the cursor, if the cursor is bigger than lenght of the dataframe, then reset it\"\"\"\n",
    "\n",
    "        self.cursor += 1\n",
    "        if self.cursor > self.df.lenght:\n",
    "            self.cursor = 0\n",
    "\n",
    "    def __close_trade(self, last_close):\n",
    "        if self.last_action == 2:\n",
    "            reward = (last_close - self.enter_price) * REWARD_FOR_PIPS \n",
    "            if reward < 0:\n",
    "                reward = reward * (TIMES_FACTOR/NEGATIVE_REWARD_DIVIDE)\n",
    "                self.closed_counter += reward / (TIMES_FACTOR/NEGATIVE_REWARD_DIVIDE)\n",
    "            if reward > 0:\n",
    "                reward = reward * TIMES_FACTOR\n",
    "                self.closed_counter += reward / TIMES_FACTOR\n",
    "            \n",
    "            \n",
    "            reward += self.POSITIVE_TIMES_REWARD * pow(reward, 3) if reward > 0 \\\n",
    "                else self.NEGATIVE_TIMES_REWARD * pow(reward, 3)\n",
    "\n",
    "        else:\n",
    "            reward = (self.enter_price - last_close) * REWARD_FOR_PIPS \n",
    "            if reward < 0:\n",
    "                reward = reward * (TIMES_FACTOR/NEGATIVE_REWARD_DIVIDE)\n",
    "                self.closed_counter += reward / (TIMES_FACTOR/NEGATIVE_REWARD_DIVIDE)\n",
    "            if reward > 0:\n",
    "                reward = reward * TIMES_FACTOR\n",
    "                self.closed_counter += reward / TIMES_FACTOR\n",
    "                \n",
    "            \n",
    "            reward += self.POSITIVE_TIMES_REWARD * pow(reward, 3) if reward > 0 \\\n",
    "                else self.NEGATIVE_TIMES_REWARD * pow(reward, 3)\n",
    "\n",
    "        self.closed_counter_list.append(self.closed_counter)\n",
    "        self.trade_counter += 1\n",
    "        return reward\n",
    "\n",
    "    def __append_last_action(self, sample: np.ndarray, action: int, last_close: float):\n",
    "        how_many = sample.shape[1]\n",
    "        decoded_action = ACTION_DECODE[action]\n",
    "\n",
    "        action_arr = (np.expand_dims(np.asarray([decoded_action for i in range(0, how_many)]), axis=1))\n",
    "\n",
    "        if action == 2 or action == 1:\n",
    "            dif = (last_close - self.enter_price)\n",
    "            pip_difference = (np.expand_dims(np.asarray([dif for i in range(0, how_many)]), axis=1))\n",
    "\n",
    "        else:\n",
    "            dif = 0\n",
    "            pip_difference = (np.expand_dims(np.asarray([dif for i in range(0, how_many)]), axis=1))\n",
    "\n",
    "        sample = np.append(sample[0], action_arr, axis=1)\n",
    "        sample = np.append(sample, pip_difference, axis=1)\n",
    "\n",
    "        return np.expand_dims(sample, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = Trevor(Dataframe())\n",
    "state_size = (NUMBER_OF_SAMPLES, 9)\n",
    "action_size = 3\n",
    "batch_size = 128 #32\n",
    "agent = DQNAgent(state_size, action_size, batch_size)\n",
    "\n",
    "#agent.save(\"./save/cartpole-ddqn.h5\")\n",
    "agent.load(\"./save/cartpole-ddqn.h5\")\n",
    "\n",
    "closed = False\n",
    "run = False\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    strt = t_lib.time()\n",
    "    \n",
    "    akce0 = 0\n",
    "    akce1 = 0\n",
    "    akce2 = 0\n",
    "    \n",
    "    for time in range(env.df.lenght):\n",
    "        action, random_action = agent.act(state)\n",
    "\n",
    "        if action > 3 or action < 0:\n",
    "            print('Got action ' + action)\n",
    "            continue\n",
    "\n",
    "        next_state, reward, closed, _ = env.step(action)\n",
    "\n",
    "        if not isinstance(next_state, np.ndarray) or not(state, np.ndarray):\n",
    "            print(next_state)\n",
    "            print('NOT NUMPY!!')\n",
    "            continue\n",
    "\n",
    "        agent.memorize(state=state, action=action, reward=reward, next_state=next_state, done=closed)\n",
    "        state = next_state\n",
    "        \n",
    "        \"\"\"\n",
    "        print(f'Actual reward = {round(reward, 1)},\\t total reward = {round(env.total_reward, 1)},'\n",
    "              f'\\t action = {action}, \\t trade_counter = {round(env.trade_counter, 1)}, '\n",
    "              f'\\t pip_counter = {round(env.closed_counter, 1)}'\n",
    "              f'\\t random_action = {random_action}'\n",
    "              f'\\t candle_number = {time}')\n",
    "        \"\"\"\n",
    "        if action == 0:\n",
    "            akce0 += 1\n",
    "        if action == 1:\n",
    "            akce1 += 1\n",
    "        if action == 2:\n",
    "            akce2 += 1\n",
    "        \n",
    "        # print(\"Actual reward = {}\\t, total reward = {},\\t action = {}\\t trade_counter = {}\\t pip_counter = {}\".format(round(reward, 1), round(env.total_reward, 1), action, round(env.trade_counter, 1), round(env.closed_counter, 1)))\n",
    "        if closed and reward > 20 * TIMES_FACTOR:\n",
    "            agent.update_target_model()\n",
    "            \"\"\"\n",
    "            print(\"episode: {}/{}, score: {}, e: {}, lr: {}\"\n",
    "                  .format(e, EPISODES, time, round(agent.epsilon, 2)), round(agent.learning_rate, 2))\n",
    "            \"\"\"\n",
    "            print('More than 20 on candle {}'.format(time))\n",
    "        \n",
    "        if len(agent.memory) > batch_size:\n",
    "            # agent.replay(batch_size)\n",
    "            if not run:\n",
    "                thr_list = [Thread(target=agent.replay) for _ in range(1)]\n",
    "                for thr in thr_list:\n",
    "                    thr.start()\n",
    "                    t_lib.sleep(1)\n",
    "                    \n",
    "                thr_list = [Thread(target=agent.train_from_iterations) for _ in range(4)]\n",
    "                for thr in thr_list:\n",
    "                    thr.start()\n",
    "                    t_lib.sleep(1)\n",
    "                \n",
    "                run = True\n",
    "                \n",
    "    # clear_output()\n",
    "    env.plot(title='total reward ={};  e = {}, lr={}, episode = {}'.format(round(env.total_reward, 2), round(agent.epsilon, 4), round(agent.learning_rate, 7), e))\n",
    "    env.reset_closed_list()\n",
    "    print('Waiting to train the whole dataset')\n",
    "    print(\"action0: \", akce0,\"\\naction1: \", akce1,\"\\naction2: \", akce2)\n",
    "    while not len(agent.sample_memory) == 0:\n",
    "        pass\n",
    "    agent.set_learning_rate()\n",
    "    print('DONE, lets roll!!')\n",
    "    agent.save(\"./save/cartpole-ddqn.h5\")\n",
    "    print(round(t_lib.time() - strt, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
