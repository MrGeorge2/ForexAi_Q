{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, CuDNNLSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import time as t_lib\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "DATAFRAME_NAME = 'EURUSD_m15_Ask_ready.csv'\n",
    "NUMBER_OF_SAMPLES = 100\n",
    "\n",
    "EPISODES = 5000\n",
    "TICQTY_MAX = 55000\n",
    "HOLD_REWARD = -0.1\n",
    "REWARD_FOR_PIPS = 10000\n",
    "TIMES_FACTOR = 1.5\n",
    "\n",
    "ACTION_DECODE = {\n",
    "    0: 0,\n",
    "    1: 0.5,\n",
    "    2: 1,\n",
    "}\n",
    "\n",
    "\n",
    "class Dataframe:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._dataframe = self._load()\n",
    "        self.__scaler = MinMaxScaler()\n",
    "\n",
    "    @property\n",
    "    def lenght(self):\n",
    "        return len(self._dataframe.index) - NUMBER_OF_SAMPLES\n",
    "\n",
    "    def get(self, sample_number):\n",
    "        if sample_number > self.lenght or sample_number < 0:\n",
    "            raise ValueError(\"Sample number out of range (0 - {})\".format(self.lenght))\n",
    "\n",
    "        start_index = sample_number\n",
    "        end_index = start_index + NUMBER_OF_SAMPLES\n",
    "\n",
    "        df_sample = self._dataframe[start_index: end_index]\n",
    "\n",
    "        last_open = df_sample.at[df_sample.index[-1], 'open']\n",
    "        last_close = df_sample.at[df_sample.index[-1], 'close']\n",
    "\n",
    "        df_sample = df_sample[['open', 'close', 'high', 'low', 'tickqty']].values\n",
    "        df_sample = self._scale(df_sample, start=0, end=4)\n",
    "        return np.expand_dims(df_sample, axis=0), last_open, last_close\n",
    "\n",
    "    @staticmethod\n",
    "    def _load():\n",
    "        \"\"\" Creating relative path and then loading the df_path \"\"\"\n",
    "        \"\"\"\n",
    "        df_path = os.path.join(os.path.dirname(os.path.abspath(__file__)) +\n",
    "                               os.path.normpath('/dfs/{}'.format(DATAFRAME_NAME)))\n",
    "        \"\"\"\n",
    "        df_path = './dfs/{}'.format(DATAFRAME_NAME)\n",
    "        df = pd.read_csv(\n",
    "            df_path,\n",
    "            dtype={\n",
    "                'datetime'\n",
    "                'open': np.float32,\n",
    "                'close': np.float32,\n",
    "                'high': np.float32,\n",
    "                'low': np.float32,\n",
    "                'tickqty': np.float32,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        df['tickqty'] = df['tickqty'] / TICQTY_MAX\n",
    "        return df\n",
    "\n",
    "    def _scale(self, array: np.ndarray, start: int, end: int):\n",
    "        columns = array.T[start: end].T\n",
    "\n",
    "        self.__scaler.fit(columns)\n",
    "        scaled_cols = self.__scaler.transform(columns).T\n",
    "        array.T[start:end] = scaled_cols\n",
    "        return array\n",
    "\n",
    "\n",
    "class Trevor:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "        self.cursor = 0\n",
    "        self.enter_price = 0\n",
    "        self.last_action = 0\n",
    "\n",
    "        self.closed_counter = 0\n",
    "        self.total_reward = 0\n",
    "        self.trade_counter = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.cursor = 0\n",
    "        self.enter_price = 0\n",
    "        self.last_action = 0\n",
    "        self.closed_counter = 0\n",
    "        self.trade_counter = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        sample, last_open, last_close = self.df.get(self.cursor)\n",
    "        sample = self.__append_last_action(sample=sample, action=action)\n",
    "\n",
    "        reward, closing_trade = self.__process_action(action=action, last_open=last_open, last_close=last_close)\n",
    "        self.__increment_cursor()\n",
    "\n",
    "        return sample, reward, closing_trade, ''\n",
    "\n",
    "    def get_total_reward(self):\n",
    "        return self.total_reward\n",
    "\n",
    "    def __process_action(self, action, last_open, last_close):\n",
    "        if action < 0 or action > 2:\n",
    "            raise ValueError('Action have to be inrage (0 - 2) got {action}')\n",
    "\n",
    "        closing_trade = False\n",
    "\n",
    "        # \"\"\" CLOSING POSITION \"\"\"\n",
    "        if (self.last_action == 2 and action == 0) or (self.last_action == 1 and action == 0):\n",
    "            reward = self.__close_trade(last_close=last_close)\n",
    "            closing_trade = True\n",
    "\n",
    "        # \"\"\" CLOSING POSITION AND GOING TO DIFFERENT POSITION \"\"\"\n",
    "        elif (self.last_action == 2 and action == 1) or (self.last_action == 1 and action == 2):\n",
    "            reward = self.__close_trade(last_close=last_close)\n",
    "            self.enter_price = last_open\n",
    "            closing_trade = True\n",
    "\n",
    "        # \"\"\" HOLDING OPENED POSITION  \"\"\"\n",
    "        elif (self.last_action == 2 and action == 2) or (self.last_action == 1 and action == 1):\n",
    "            if self.last_action == 2:\n",
    "                reward = (last_close - self.enter_price) * REWARD_FOR_PIPS\n",
    "\n",
    "            else:\n",
    "                reward = (self.enter_price - last_close) * REWARD_FOR_PIPS\n",
    "\n",
    "        # \"\"\" OPENING POSITION  \"\"\"\n",
    "        elif (self.last_action == 0 and action == 1) or (self.last_action == 0 and action == 2):\n",
    "            if action == 1:\n",
    "                self.enter_price = last_open\n",
    "\n",
    "            else:\n",
    "                self.enter_price = last_open\n",
    "            reward = HOLD_REWARD\n",
    "\n",
    "        # \"\"\" HOLD \"\"\"\n",
    "        elif self.last_action == 0 and action == 0:\n",
    "            reward = HOLD_REWARD\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Last action = {self.last_action} and actual_action = {action}')\n",
    "\n",
    "        self.last_action = action\n",
    "        self.total_reward += reward\n",
    "        return reward, closing_trade\n",
    "\n",
    "    def __increment_cursor(self):\n",
    "        \"\"\" Incrementing the cursor, if the cursor is bigger than lenght of the dataframe, then reset it\"\"\"\n",
    "\n",
    "        self.cursor += 1\n",
    "        if self.cursor > self.df.lenght:\n",
    "            self.reset()\n",
    "\n",
    "    def __close_trade(self, last_close):\n",
    "        if self.last_action == 2:\n",
    "            reward = (last_close - self.enter_price) * REWARD_FOR_PIPS * TIMES_FACTOR\n",
    "\n",
    "        else:\n",
    "            reward = (self.enter_price - last_close) * REWARD_FOR_PIPS * TIMES_FACTOR\n",
    "\n",
    "        self.closed_counter += reward / TIMES_FACTOR\n",
    "        self.trade_counter += 1\n",
    "        return reward\n",
    "\n",
    "    def __append_last_action(self, sample: np.ndarray, action: int):\n",
    "        how_many = sample.shape[1]\n",
    "        action = ACTION_DECODE[action]\n",
    "\n",
    "        action_arr = (np.expand_dims(np.asarray([action for i in range(0, how_many)]), axis=1))\n",
    "\n",
    "        return np.expand_dims(np.append(sample[0], action_arr, axis=1), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 0.01  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.9993\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    \"\"\"Huber loss for Q Learning\n",
    "\n",
    "    References: https://en.wikipedia.org/wiki/Huber_loss\n",
    "                https://www.tensorflow.org/api_docs/python/tf/losses/huber_loss\n",
    "    \"\"\"\n",
    "\n",
    "    def _huber_loss(self, y_true, y_pred, clip_delta=1.0):\n",
    "        error = y_true - y_pred\n",
    "        cond = K.abs(error) <= clip_delta\n",
    "\n",
    "        squared_loss = 0.5 * K.square(error)\n",
    "        quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
    "\n",
    "        return K.mean(tf.where(cond, squared_loss, quadratic_loss))\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(CuDNNLSTM(units=32, return_sequences=True, input_shape=self.state_size))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(CuDNNLSTM(units=16, return_sequences=False))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss=self._huber_loss,\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if not isinstance(state, np.ndarray):\n",
    "            return 0\n",
    "\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state, steps=1)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def predict(self, state):\n",
    "        act_values = self.model.predict(state, steps=1)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            if not isinstance(state, np.ndarray):\n",
    "                continue\n",
    "\n",
    "            target = self.model.predict(state, steps=1, verbose=0)\n",
    "            if done and reward > 0:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                # a = self.model.predict(next_state)[0]\n",
    "                t = self.target_model.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.gamma * np.amax(t)\n",
    "                # target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "\n",
    "def eval_test(state_size, action_size):\n",
    "    envv = Trevor(Dataframe())\n",
    "\n",
    "    agentt = DQNAgent(state_size, action_size)\n",
    "    agentt.load(\"./save/cartpole-ddqn.h5\")\n",
    "\n",
    "    sample = envv.reset()\n",
    "\n",
    "    for i in range(envv.df.lenght):\n",
    "        acc = agentt.predict(sample)\n",
    "        sample, rewardd, closedd, _ = envv.step(acc)\n",
    "        print('Actual reward = {},\\t total reward = {},\\t action = {}'.format(round(rewardd, 3),\n",
    "                                                                              round(envv.get_total_reward(), 3),\n",
    "                                                                              acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From <ipython-input-9-f3befd6cb7a6>:221: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Actual = -0.1\t total_reward = -0.1\t action = 0\t trade_counter = 0\t pip_counter = 0\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Actual = -0.1\t total_reward = -0.2\t action = 1\t trade_counter = 0\t pip_counter = 0\n",
      "Actual = -8.6\t total_reward = -8.8\t action = 1\t trade_counter = 0\t pip_counter = 0\n",
      "Actual = -13.3\t total_reward = -22.1\t action = 1\t trade_counter = 0\t pip_counter = 0\n",
      "Actual = -9.1\t total_reward = -31.2\t action = 1\t trade_counter = 0\t pip_counter = 0\n",
      "Actual = -13.1\t total_reward = -44.3\t action = 1\t trade_counter = 0\t pip_counter = 0\n",
      "Actual = -13.8\t total_reward = -58.1\t action = 1\t trade_counter = 0\t pip_counter = 0\n",
      "Actual = -10.7\t total_reward = -68.8\t action = 1\t trade_counter = 0\t pip_counter = 0\n",
      "Actual = 0.9\t total_reward = -67.9\t action = 1\t trade_counter = 0\t pip_counter = 0\n",
      "Actual = -19.1\t total_reward = -87.0\t action = 1\t trade_counter = 0\t pip_counter = 0\n",
      "Actual = -19.9\t total_reward = -106.9\t action = 1\t trade_counter = 0\t pip_counter = 0\n",
      "Actual = -20.9\t total_reward = -127.8\t action = 1\t trade_counter = 0\t pip_counter = 0\n",
      "Actual = -21.3\t total_reward = -149.1\t action = 1\t trade_counter = 0\t pip_counter = 0\n",
      "Actual = -19.9\t total_reward = -169.0\t action = 1\t trade_counter = 0\t pip_counter = 0\n",
      "Actual = -23.4\t total_reward = -192.4\t action = 1\t trade_counter = 0\t pip_counter = 0\n",
      "Actual = -16.7\t total_reward = -209.1\t action = 1\t trade_counter = 0\t pip_counter = 0\n",
      "Actual = -10.7\t total_reward = -219.8\t action = 1\t trade_counter = 0\t pip_counter = 0\n",
      "Actual = -11.9\t total_reward = -231.7\t action = 1\t trade_counter = 0\t pip_counter = 0\n",
      "Actual = -10.6\t total_reward = -242.3\t action = 1\t trade_counter = 0\t pip_counter = 0\n",
      "Actual = -9.1\t total_reward = -251.4\t action = 1\t trade_counter = 0\t pip_counter = 0\n",
      "Actual = 2.5\t total_reward = -248.9\t action = 1\t trade_counter = 0\t pip_counter = 0\n",
      "Actual = 4.6\t total_reward = -244.3\t action = 1\t trade_counter = 0\t pip_counter = 0\n",
      "Actual = 14.1\t total_reward = -230.2\t action = 2\t trade_counter = 1\t pip_counter = 9.4\n",
      "episode: 0/5000, score: 22, e: 0.01\n",
      "Actual = -19.5\t total_reward = -249.7\t action = 1\t trade_counter = 2\t pip_counter = -3.6\n",
      "Actual = 6.1\t total_reward = -243.6\t action = 1\t trade_counter = 2\t pip_counter = -3.6\n",
      "Actual = 1.6\t total_reward = -242.0\t action = 1\t trade_counter = 2\t pip_counter = -3.6\n",
      "Actual = -3.6\t total_reward = -245.6\t action = 1\t trade_counter = 2\t pip_counter = -3.6\n",
      "Actual = 6.3\t total_reward = -239.3\t action = 1\t trade_counter = 2\t pip_counter = -3.6\n",
      "Actual = 2.0\t total_reward = -237.3\t action = 1\t trade_counter = 2\t pip_counter = -3.6\n",
      "Actual = -1.0\t total_reward = -238.3\t action = 1\t trade_counter = 2\t pip_counter = -3.6\n",
      "Actual = -7.5\t total_reward = -245.8\t action = 1\t trade_counter = 2\t pip_counter = -3.6\n",
      "Actual = -15.8\t total_reward = -261.6\t action = 1\t trade_counter = 2\t pip_counter = -3.6\n",
      "Actual = -18.5\t total_reward = -280.1\t action = 1\t trade_counter = 2\t pip_counter = -3.6\n",
      "Actual = -24.3\t total_reward = -304.4\t action = 2\t trade_counter = 3\t pip_counter = -19.8\n",
      "Actual = -3.75\t total_reward = -308.15\t action = 1\t trade_counter = 4\t pip_counter = -22.3\n",
      "Actual = 1.8\t total_reward = -306.35\t action = 2\t trade_counter = 5\t pip_counter = -21.1\n",
      "episode: 0/5000, score: 35, e: 0.01\n",
      "Actual = 54.0\t total_reward = -252.35\t action = 1\t trade_counter = 6\t pip_counter = 14.9\n",
      "episode: 0/5000, score: 36, e: 0.01\n",
      "Actual = -42.45\t total_reward = -294.8\t action = 2\t trade_counter = 7\t pip_counter = -13.4\n",
      "Actual = 1.2\t total_reward = -293.6\t action = 2\t trade_counter = 7\t pip_counter = -13.4\n",
      "Actual = -13.35\t total_reward = -306.95\t action = 1\t trade_counter = 8\t pip_counter = -22.3\n",
      "Actual = 13.2\t total_reward = -293.75\t action = 2\t trade_counter = 9\t pip_counter = -13.5\n",
      "episode: 0/5000, score: 40, e: 0.01\n",
      "Actual = 20.55\t total_reward = -273.2\t action = 1\t trade_counter = 10\t pip_counter = 0.2\n",
      "episode: 0/5000, score: 41, e: 0.01\n",
      "Actual = 1.3\t total_reward = -271.9\t action = 1\t trade_counter = 10\t pip_counter = 0.2\n",
      "Actual = -4.3\t total_reward = -276.2\t action = 1\t trade_counter = 10\t pip_counter = 0.2\n",
      "Actual = -3.45\t total_reward = -279.65\t action = 0\t trade_counter = 11\t pip_counter = -2.1\n",
      "Actual = -0.1\t total_reward = -279.75\t action = 2\t trade_counter = 11\t pip_counter = -2.1\n",
      "Actual = 4.7\t total_reward = -275.05\t action = 2\t trade_counter = 11\t pip_counter = -2.1\n",
      "Actual = 3.6\t total_reward = -271.45\t action = 1\t trade_counter = 12\t pip_counter = 0.3\n",
      "episode: 0/5000, score: 47, e: 0.01\n",
      "Actual = 2.4\t total_reward = -269.05\t action = 2\t trade_counter = 13\t pip_counter = 1.9\n",
      "episode: 0/5000, score: 48, e: 0.01\n",
      "Actual = 7.5\t total_reward = -261.55\t action = 1\t trade_counter = 14\t pip_counter = 6.9\n",
      "episode: 0/5000, score: 49, e: 0.01\n",
      "Actual = 0.1\t total_reward = -261.45\t action = 1\t trade_counter = 14\t pip_counter = 6.9\n",
      "Actual = -1.2\t total_reward = -262.65\t action = 1\t trade_counter = 14\t pip_counter = 6.9\n",
      "Actual = 2.55\t total_reward = -260.1\t action = 2\t trade_counter = 15\t pip_counter = 8.6\n",
      "episode: 0/5000, score: 52, e: 0.01\n",
      "Actual = -3.3\t total_reward = -263.4\t action = 1\t trade_counter = 16\t pip_counter = 6.4\n",
      "Actual = 12.0\t total_reward = -251.4\t action = 2\t trade_counter = 17\t pip_counter = 14.4\n",
      "episode: 0/5000, score: 54, e: 0.01\n",
      "Actual = -10.65\t total_reward = -262.05\t action = 1\t trade_counter = 18\t pip_counter = 7.3\n",
      "Actual = -11.55\t total_reward = -273.6\t action = 2\t trade_counter = 19\t pip_counter = -0.4\n",
      "Actual = 12.3\t total_reward = -261.3\t action = 1\t trade_counter = 20\t pip_counter = 7.8\n",
      "episode: 0/5000, score: 57, e: 0.01\n",
      "Actual = 2.5\t total_reward = -258.8\t action = 1\t trade_counter = 20\t pip_counter = 7.8\n",
      "Actual = -0.7\t total_reward = -259.5\t action = 1\t trade_counter = 20\t pip_counter = 7.8\n",
      "Actual = -0.2\t total_reward = -259.7\t action = 1\t trade_counter = 20\t pip_counter = 7.8\n",
      "Actual = -1.7\t total_reward = -261.4\t action = 1\t trade_counter = 20\t pip_counter = 7.8\n",
      "Actual = 1.5\t total_reward = -259.9\t action = 2\t trade_counter = 21\t pip_counter = 8.8\n",
      "episode: 0/5000, score: 62, e: 0.01\n",
      "Actual = -8.1\t total_reward = -268.0\t action = 1\t trade_counter = 22\t pip_counter = 3.4\n",
      "Actual = 9.0\t total_reward = -259.0\t action = 2\t trade_counter = 23\t pip_counter = 9.4\n",
      "episode: 0/5000, score: 64, e: 0.01\n",
      "Actual = 7.05\t total_reward = -251.95\t action = 1\t trade_counter = 24\t pip_counter = 14.1\n",
      "episode: 0/5000, score: 65, e: 0.01\n",
      "Actual = -5.55\t total_reward = -257.5\t action = 2\t trade_counter = 25\t pip_counter = 10.4\n",
      "Actual = -7.35\t total_reward = -264.85\t action = 1\t trade_counter = 26\t pip_counter = 5.5\n",
      "Actual = 4.5\t total_reward = -260.35\t action = 2\t trade_counter = 27\t pip_counter = 8.5\n",
      "episode: 0/5000, score: 68, e: 0.01\n",
      "Actual = 0.75\t total_reward = -259.6\t action = 1\t trade_counter = 28\t pip_counter = 9.0\n",
      "episode: 0/5000, score: 69, e: 0.01\n",
      "Actual = -4.05\t total_reward = -263.65\t action = 2\t trade_counter = 29\t pip_counter = 6.3\n",
      "Actual = -5.7\t total_reward = -269.35\t action = 1\t trade_counter = 30\t pip_counter = 2.5\n",
      "Actual = 2.1\t total_reward = -267.25\t action = 2\t trade_counter = 31\t pip_counter = 3.9\n",
      "episode: 0/5000, score: 72, e: 0.01\n",
      "Actual = 7.2\t total_reward = -260.05\t action = 2\t trade_counter = 31\t pip_counter = 3.9\n",
      "Actual = 9.9\t total_reward = -250.15\t action = 2\t trade_counter = 31\t pip_counter = 3.9\n",
      "Actual = 15.1\t total_reward = -235.05\t action = 2\t trade_counter = 31\t pip_counter = 3.9\n",
      "Actual = 14.3\t total_reward = -220.75\t action = 2\t trade_counter = 31\t pip_counter = 3.9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-61c5c71be4b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./save/cartpole-ddqn.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-f3befd6cb7a6>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0;31m# a = self.model.predict(next_state)[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0;31m# target[0][action] = reward + self.gamma * t[np.argmax(a)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = Trevor(Dataframe())\n",
    "state_size = (NUMBER_OF_SAMPLES, 6)\n",
    "action_size = 3\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "agent.load(\"./save/cartpole-ddqn.h5\")\n",
    "\n",
    "closed = False\n",
    "batch_size = 32\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "\n",
    "    for time in range(env.df.lenght):\n",
    "        action = agent.act(state)\n",
    "\n",
    "        if action > 3 or action < 0:\n",
    "            print('Got action ' + action)\n",
    "            continue\n",
    "\n",
    "        next_state, reward, closed, _ = env.step(action)\n",
    "\n",
    "        if not isinstance(next_state, np.ndarray) or not(state, np.ndarray):\n",
    "            print(next_state)\n",
    "            print('NOT NUMPY!!')\n",
    "            continue\n",
    "\n",
    "        agent.memorize(state=state, action=action, reward=reward, next_state=next_state, done=closed)\n",
    "        state = next_state\n",
    "        \n",
    "        \"\"\"\n",
    "        print(f'Actual reward = {round(reward, 2)},\\t total reward = {round(env.total_reward, 2)},'\n",
    "              f'\\t action = {action}, \\t trade_counter = {round(env.trade_counter, 2)}, '\n",
    "              f'\\t pip_counter = {env.closed_counter}')\n",
    "        \"\"\"\n",
    "        print('Actual = {}\\t total_reward = {}\\t action = {}\\t trade_counter = {}\\t pip_counter = {}'.format(round(reward, 2), round(env.total_reward, 2), action, env.trade_counter, round(env.closed_counter, 2)))\n",
    "\n",
    "        if closed and reward > 0:\n",
    "            agent.update_target_model()\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                  .format(e, EPISODES, time, agent.epsilon))\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "\n",
    "    agent.save(\"./save/cartpole-ddqn.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
