{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, CuDNNLSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import time as t_lib\n",
    "import tensorflow as tf\n",
    "from threading import Thread\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFRAME_NAME = 'EURUSD_m15_Ask_ready.csv'\n",
    "NUMBER_OF_SAMPLES = 50\n",
    "\n",
    "EPISODES = 5000\n",
    "TICQTY_MAX = 55000\n",
    "HOLD_REWARD = -1\n",
    "OPEN_TRADE_REWARD = 0\n",
    "CLOSING_TRADE_WITH_OPENING = 20\n",
    "DIVIDE_PRICE_UNDER_LOCAL_MINIMA = 10\n",
    "REWARD_FOR_PIPS = 10000\n",
    "TIMES_FACTOR = 5\n",
    "\n",
    "ACTION_DECODE = {\n",
    "    0: 0,\n",
    "    1: 0.5,\n",
    "    2: 1,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class Dataframe:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._dataframe = self._load()[:2000]\n",
    "        self.__scaler = MinMaxScaler()\n",
    "\n",
    "    @property\n",
    "    def lenght(self):\n",
    "        return len(self._dataframe.index) - NUMBER_OF_SAMPLES\n",
    "\n",
    "    def get(self, sample_number):\n",
    "        if sample_number > self.lenght or sample_number < 0:\n",
    "            raise ValueError(\"Sample number out of range (0 - {self.lenght})\")\n",
    "\n",
    "        start_index = sample_number\n",
    "        end_index = start_index + NUMBER_OF_SAMPLES\n",
    "\n",
    "        df_sample = self._dataframe[start_index: end_index]\n",
    "\n",
    "        last_open = df_sample.at[df_sample.index[-1], 'open']\n",
    "        last_close = df_sample.at[df_sample.index[-1], 'close']\n",
    "\n",
    "        df_sample = df_sample[['open', 'close', 'high', 'low', 'tickqty', 'hours', 'minutes']].values\n",
    "        df_sample = self._scale(df_sample, start=0, end=4)\n",
    "        return np.expand_dims(df_sample, axis=0), last_open, last_close\n",
    "\n",
    "    @staticmethod\n",
    "    def _load():\n",
    "        \"\"\" Creating relative path and then loading the df_path \"\"\"\n",
    "        \"\"\"\n",
    "        df_path = os.path.join(os.path.dirname(os.path.abspath(__file__)) +\n",
    "                               os.path.normpath(f'/dfs/{cfg.DATAFRAME_NAME}'))\n",
    "        \"\"\"\n",
    "        df_path = './dfs/{}'.format(DATAFRAME_NAME)\n",
    "        df = pd.read_csv(\n",
    "            df_path,\n",
    "            dtype={\n",
    "                'datetime'\n",
    "                'open': np.float32,\n",
    "                'close': np.float32,\n",
    "                'high': np.float32,\n",
    "                'low': np.float32,\n",
    "                'tickqty': np.float32,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # df['hours']= pd.to_datetime(df['datetime'], format='%Y%m%d %H:%M:%S.%f').dt.hour / 24\n",
    "        df['hours'] = pd.to_datetime(df['date'], format='%m-%d-%Y %H:%M:%S').dt.hour / 24\n",
    "        df['minutes'] = pd.to_datetime(df['date'], format='%m-%d-%Y %H:%M:%S').dt.minute / 64\n",
    "        df['tickqty'] = df['tickqty'] / TICQTY_MAX\n",
    "        return df\n",
    "\n",
    "    def _scale(self, array: np.ndarray, start: int, end: int):\n",
    "        columns = array.T[start: end].T\n",
    "\n",
    "        self.__scaler.fit(columns)\n",
    "        scaled_cols = self.__scaler.transform(columns).T\n",
    "        array.T[start:end] = scaled_cols\n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, batch_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=20000)\n",
    "        self.sample_memory = deque()\n",
    "        \n",
    "        self.gamma = 0.99  # discount rate\n",
    "        \n",
    "        self.epsilon = 0.95 # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.9992\n",
    "        \n",
    "        self.learning_rate = 0.002\n",
    "        self.learning_rate_decay = 0.9992\n",
    "        self.learning_rate_min = 0.002\n",
    "        \n",
    "        self.batch_size_samples = 500\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    \"\"\"Huber loss for Q Learning\n",
    "\n",
    "    References: https://en.wikipedia.org/wiki/Huber_loss\n",
    "                https://www.tensorflow.org/api_docs/python/tf/losses/huber_loss\n",
    "    \"\"\"\n",
    "\n",
    "    def _huber_loss(self, y_true, y_pred, clip_delta=1.0):\n",
    "        error = y_true - y_pred\n",
    "        cond = K.abs(error) <= clip_delta\n",
    "\n",
    "        squared_loss = 0.5 * K.square(error)\n",
    "        quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
    "\n",
    "        return K.mean(tf.where(cond, squared_loss, quadratic_loss))\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(CuDNNLSTM(units=50, return_sequences=True, input_shape=self.state_size))\n",
    "\n",
    "        model.add(CuDNNLSTM(units=33, return_sequences=False))\n",
    "        \n",
    "        model.add(Dense(units=22, activation='relu'))\n",
    "\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss=self._huber_loss,\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        self.sample_memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train_from_iterations(self):\n",
    "        while True:\n",
    "            if len(self.sample_memory) != 0:\n",
    "                state, action, reward, next_state, done = self.sample_memory.popleft()\n",
    "                self.train(state, action, reward, next_state, done)\n",
    "\n",
    "    def train(self, state, action, reward, next_state, done):\n",
    "        target = self.model.predict(state, steps=1, verbose=0)\n",
    "        if done and reward > 80 * TIMES_FACTOR:\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            # a = self.model.predict(next_state)[0]\n",
    "            t = self.target_model.predict(next_state)[0]\n",
    "            target[0][action] = reward + self.gamma * np.amax(t)\n",
    "            # target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
    "        self.model.fit(state, target, epochs=1, verbose=0)\n",
    "        # print('done')\n",
    "\n",
    "    def act(self, state):\n",
    "        if not isinstance(state, np.ndarray):\n",
    "            return 0\n",
    "\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size), True\n",
    "        act_values = self.model.predict(state, steps=1)\n",
    "        return np.argmax(act_values[0]), False  # returns action\n",
    "\n",
    "    def predict(self, state):\n",
    "        act_values = self.model.predict(state, steps=1)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self):\n",
    "        while True:\n",
    "            minibatch = random.sample(self.memory, self.batch_size)\n",
    "            for state, action, reward, next_state, done in minibatch:\n",
    "                if not isinstance(state, np.ndarray):\n",
    "                    continue\n",
    "                \n",
    "                self.train(state, action, reward, next_state, done)\n",
    "                \"\"\"\n",
    "                target = self.model.predict(state, steps=1, verbose=0)\n",
    "                if done and reward > 80 * TIMES_FACTOR:\n",
    "                    target[0][action] = reward\n",
    "                else:\n",
    "                    # a = self.model.predict(next_state)[0]\n",
    "                    t = self.target_model.predict(next_state)[0]\n",
    "                    target[0][action] = reward + self.gamma * np.amax(t)\n",
    "                    # target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
    "                self.model.fit(state, target, epochs=1, verbose=0)\n",
    "                \"\"\"\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "            else:\n",
    "                self.epsilon = self.epsilon_min\n",
    "            \n",
    "            if self.learning_rate > self.learning_rate_min:\n",
    "                self.learning_rate *= self.learning_rate_decay\n",
    "            else:\n",
    "                self.learning_rate = self.learning_rate_min\n",
    "            # print('done')\n",
    "    \n",
    "    def set_learning_rate(self):\n",
    "        K.set_value(self.model.optimizer.lr, self.learning_rate)  # set new lr\n",
    "        K.set_value(self.target_model.optimizer.lr, self.learning_rate)  # set new lr\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "        self.model._make_predict_function()\n",
    "        self.model._make_test_function()\n",
    "        self.model._make_train_function()\n",
    "\n",
    "        self.target_model.load_weights(name)\n",
    "        self.target_model._make_predict_function()\n",
    "        self.target_model._make_test_function()\n",
    "        self.target_model._make_train_function()\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trevor:\n",
    "    POSITIVE_TIMES_REWARD = 0.00001\n",
    "    NEGATIVE_TIMES_REWARD = 0.00001\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "        self.cursor = 0\n",
    "        self.enter_price = 0\n",
    "        self.local_max_price = 0\n",
    "\n",
    "        self.last_action = 0\n",
    "\n",
    "        self.closed_counter = 0\n",
    "        self.total_reward = 0\n",
    "        self.trade_counter = 0\n",
    "\n",
    "        self.closed_counter_list = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.cursor = 0\n",
    "        self.enter_price = 0\n",
    "        self.last_action = 0\n",
    "        self.closed_counter = 0\n",
    "        self.trade_counter = 0\n",
    "        self.total_reward = 0\n",
    "        # self.reset_closed_list()\n",
    "\n",
    "        return self.step(0)[0]\n",
    "\n",
    "    def step(self, action):\n",
    "        sample, last_open, last_close = self.df.get(self.cursor)\n",
    "\n",
    "        reward, closing_trade = self.__process_action(action=action, last_close=last_close)\n",
    "        sample = self.__append_last_action(sample=sample, action=action, last_close=last_close)\n",
    "\n",
    "        self.__increment_cursor()\n",
    "\n",
    "        return sample, reward, closing_trade, ''\n",
    "\n",
    "    def get_total_reward(self):\n",
    "        return self.total_reward\n",
    "\n",
    "    def reset_closed_list(self):\n",
    "        self.closed_counter_list = []\n",
    "\n",
    "    def plot(self, title):\n",
    "        x = list(range(1, len(self.closed_counter_list) + 1))\n",
    "        pyplot.plot(x, self.closed_counter_list)\n",
    "        pyplot.title(str(title))\n",
    "        pyplot.show()\n",
    "\n",
    "    def __process_action(self, action, last_close):\n",
    "        if action < 0 or action > 2:\n",
    "            raise ValueError('Action have to be inrage (0 - 2) got {action}')\n",
    "\n",
    "        closing_trade = False\n",
    "\n",
    "        # \"\"\" CLOSING POSITION \"\"\"\n",
    "        if (self.last_action == 2 and action == 0) or (self.last_action == 1 and action == 0):\n",
    "            reward = self.__close_trade(last_close=last_close)\n",
    "            closing_trade = True\n",
    "\n",
    "        # \"\"\" CLOSING POSITION AND GOING TO DIFFERENT POSITION \"\"\"\n",
    "        elif (self.last_action == 2 and action == 1) or (self.last_action == 1 and action == 2):\n",
    "            reward = self.__close_trade(last_close=last_close) - CLOSING_TRADE_WITH_OPENING\n",
    "            self.enter_price = last_close\n",
    "            self.local_max_price = last_close\n",
    "            closing_trade = True\n",
    "\n",
    "        # \"\"\" HOLDING OPENED POSITION  \"\"\"\n",
    "        elif (self.last_action == 2 and action == 2) or (self.last_action == 1 and action == 1):\n",
    "            if self.last_action == 2:\n",
    "                if self.local_max_price < last_close:\n",
    "                    reward = (last_close - self.enter_price) * REWARD_FOR_PIPS\n",
    "                    self.local_max_price = last_close\n",
    "\n",
    "                else:\n",
    "                    reward = (last_close - self.local_max_price) * REWARD_FOR_PIPS\n",
    "                    reward = reward / DIVIDE_PRICE_UNDER_LOCAL_MINIMA if last_close > self.enter_price \\\n",
    "                        else reward\n",
    "\n",
    "            else:\n",
    "                if self.local_max_price > last_close:\n",
    "                    reward = (self.enter_price - last_close) * REWARD_FOR_PIPS\n",
    "                    self.local_max_price = last_close\n",
    "\n",
    "                else:\n",
    "                    reward = (self.local_max_price - last_close) * REWARD_FOR_PIPS\n",
    "                    reward = reward / DIVIDE_PRICE_UNDER_LOCAL_MINIMA if last_close < self.enter_price \\\n",
    "                        else reward\n",
    "\n",
    "        # \"\"\" OPENING POSITION  \"\"\"\n",
    "        elif (self.last_action == 0 and action == 1) or (self.last_action == 0 and action == 2):\n",
    "            self.enter_price = last_close\n",
    "            self.local_max_price = last_close\n",
    "            reward = OPEN_TRADE_REWARD\n",
    "\n",
    "        # \"\"\" HOLD \"\"\"\n",
    "        elif self.last_action == 0 and action == 0:\n",
    "            reward = HOLD_REWARD\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Last action = {self.last_action} and actual_action = {action}')\n",
    "\n",
    "        self.last_action = action\n",
    "        self.total_reward += reward\n",
    "        return reward, closing_trade\n",
    "\n",
    "    def __increment_cursor(self):\n",
    "        \"\"\" Incrementing the cursor, if the cursor is bigger than lenght of the dataframe, then reset it\"\"\"\n",
    "\n",
    "        self.cursor += 1\n",
    "        if self.cursor > self.df.lenght:\n",
    "            self.cursor = 0\n",
    "\n",
    "    def __close_trade(self, last_close):\n",
    "        if self.last_action == 2:\n",
    "            reward = (last_close - self.enter_price) * REWARD_FOR_PIPS * TIMES_FACTOR\n",
    "            self.closed_counter += reward / TIMES_FACTOR\n",
    "            reward += self.POSITIVE_TIMES_REWARD * pow(reward, 3) if reward > 0 \\\n",
    "                else self.NEGATIVE_TIMES_REWARD * pow(reward, 3)\n",
    "\n",
    "        else:\n",
    "            reward = (self.enter_price - last_close) * REWARD_FOR_PIPS * TIMES_FACTOR\n",
    "            self.closed_counter += reward / TIMES_FACTOR\n",
    "            reward += self.POSITIVE_TIMES_REWARD * pow(reward, 3) if reward > 0 \\\n",
    "                else self.NEGATIVE_TIMES_REWARD * pow(reward, 3)\n",
    "\n",
    "        self.closed_counter_list.append(self.closed_counter)\n",
    "        self.trade_counter += 1\n",
    "        return reward\n",
    "\n",
    "    def __append_last_action(self, sample: np.ndarray, action: int, last_close: float):\n",
    "        how_many = sample.shape[1]\n",
    "        decoded_action = ACTION_DECODE[action]\n",
    "\n",
    "        action_arr = (np.expand_dims(np.asarray([decoded_action for i in range(0, how_many)]), axis=1))\n",
    "\n",
    "        if action == 2 or action == 1:\n",
    "            dif = (last_close - self.enter_price)\n",
    "            pip_difference = (np.expand_dims(np.asarray([dif for i in range(0, how_many)]), axis=1))\n",
    "\n",
    "        else:\n",
    "            dif = 0\n",
    "            pip_difference = (np.expand_dims(np.asarray([dif for i in range(0, how_many)]), axis=1))\n",
    "\n",
    "        sample = np.append(sample[0], action_arr, axis=1)\n",
    "        sample = np.append(sample, pip_difference, axis=1)\n",
    "\n",
    "        return np.expand_dims(sample, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hejdu\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\hejdu\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "More than 80 on candle 353\n",
      "More than 80 on candle 495\n",
      "More than 80 on candle 1723\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting to train the whole dataset\n"
     ]
    }
   ],
   "source": [
    "env = Trevor(Dataframe())\n",
    "state_size = (NUMBER_OF_SAMPLES, 9)\n",
    "action_size = 3\n",
    "batch_size = 32\n",
    "agent = DQNAgent(state_size, action_size, batch_size)\n",
    "\n",
    "# agent.save(\"./save/cartpole-ddqn.h5\")\n",
    "agent.load(\"./save/cartpole-ddqn.h5\")\n",
    "\n",
    "closed = False\n",
    "run = False\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    strt = t_lib.time()\n",
    "    \n",
    "    for time in range(env.df.lenght):\n",
    "        action, random_action = agent.act(state)\n",
    "\n",
    "        if action > 3 or action < 0:\n",
    "            print('Got action ' + action)\n",
    "            continue\n",
    "\n",
    "        next_state, reward, closed, _ = env.step(action)\n",
    "\n",
    "        if not isinstance(next_state, np.ndarray) or not(state, np.ndarray):\n",
    "            print(next_state)\n",
    "            print('NOT NUMPY!!')\n",
    "            continue\n",
    "\n",
    "        agent.memorize(state=state, action=action, reward=reward, next_state=next_state, done=closed)\n",
    "        state = next_state\n",
    "        \n",
    "        \"\"\"\n",
    "        print(f'Actual reward = {round(reward, 1)},\\t total reward = {round(env.total_reward, 1)},'\n",
    "              f'\\t action = {action}, \\t trade_counter = {round(env.trade_counter, 1)}, '\n",
    "              f'\\t pip_counter = {round(env.closed_counter, 1)}'\n",
    "              f'\\t random_action = {random_action}'\n",
    "              f'\\t candle_number = {time}')\n",
    "        \"\"\"\n",
    "        # print(\"Actual reward = {}\\t, total reward = {},\\t action = {}\\t trade_counter = {}\\t pip_counter = {}\".format(round(reward, 1), round(env.total_reward, 1), action, round(env.trade_counter, 1), round(env.closed_counter, 1)))\n",
    "        if closed and reward > 80 * TIMES_FACTOR:\n",
    "            agent.update_target_model()\n",
    "            \"\"\"\n",
    "            print(\"episode: {}/{}, score: {}, e: {}, lr: {}\"\n",
    "                  .format(e, EPISODES, time, round(agent.epsilon, 2)), round(agent.learning_rate, 2))\n",
    "            \"\"\"\n",
    "            print('More than 80 on candle {}'.format(time))\n",
    "        \n",
    "        if len(agent.memory) > batch_size:\n",
    "            # agent.replay(batch_size)\n",
    "            if not run:\n",
    "                thr_list = [Thread(target=agent.replay) for _ in range(1)]\n",
    "                for thr in thr_list:\n",
    "                    thr.start()\n",
    "                    t_lib.sleep(1)\n",
    "                    \n",
    "                thr_list = [Thread(target=agent.train_from_iterations) for _ in range(5)]\n",
    "                for thr in thr_list:\n",
    "                    thr.start()\n",
    "                    t_lib.sleep(1)\n",
    "                \n",
    "                run = True\n",
    "                \n",
    "    # clear_output()\n",
    "    env.plot(title='total reward ={};  e = {}, lr={}, episode = {}'.format(round(env.total_reward, 2), round(agent.epsilon, 4), round(agent.learning_rate, 5), e))\n",
    "    env.reset_closed_list()\n",
    "    print('Waiting to train the whole dataset')\n",
    "    while not len(agent.sample_memory) == 0:\n",
    "        pass\n",
    "    agent.set_learning_rate()\n",
    "    print('DONE, lets roll!!')\n",
    "    agent.save(\"./save/cartpole-ddqn.h5\")\n",
    "    print(round(t_lib.time() - strt, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
