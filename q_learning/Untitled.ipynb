{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, CuDNNLSTM, Conv1D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import time as t_lib\n",
    "import tensorflow as tf\n",
    "from threading import Thread\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "DATAFRAME_NAME = 'triangle.csv'\n",
    "NUMBER_OF_SAMPLES = 20\n",
    "\n",
    "EPISODES = 5000\n",
    "TICQTY_MAX = 55000\n",
    "HOLD_REWARD = -50\n",
    "OPEN_TRADE_REWARD = 0\n",
    "CLOSING_TRADE_WITH_OPENING = 100  #20\n",
    "DIVIDE_PRICE_UNDER_LOCAL_MINIMA = 1  #10\n",
    "REWARD_FOR_PIPS = 10000\n",
    "TIMES_FACTOR = 1 #10\n",
    "NEGATIVE_REWARD_DIVIDE = 1 # kolikrat bude mensi times factor pri zaporne odmene\n",
    "\n",
    "ACTION_DECODE = {\n",
    "    0: 0,\n",
    "    1: 0.5,\n",
    "    2: 1,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Dataframe:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._dataframe = self._load()[0:2000]\n",
    "        self.__scaler = MinMaxScaler()\n",
    "\n",
    "    @property\n",
    "    def lenght(self):\n",
    "        return len(self._dataframe.index) - NUMBER_OF_SAMPLES\n",
    "\n",
    "    def get(self, sample_number):\n",
    "        if sample_number > self.lenght or sample_number < 0:\n",
    "            raise ValueError(\"Sample number out of range (0 - {self.lenght})\")\n",
    "\n",
    "        start_index = sample_number\n",
    "        end_index = start_index + NUMBER_OF_SAMPLES\n",
    "\n",
    "        df_sample = self._dataframe[start_index: end_index]\n",
    "\n",
    "        last_open = df_sample.at[df_sample.index[-1], 'open']\n",
    "        last_close = df_sample.at[df_sample.index[-1], 'close']\n",
    "\n",
    "        df_sample = df_sample[['open', 'close', 'high', 'low', 'tickqty', 'hours', 'minutes']].values\n",
    "        df_sample = self._scale(df_sample, start=0, end=4)\n",
    "        return np.expand_dims(df_sample, axis=0), last_open, last_close\n",
    "\n",
    "    @staticmethod\n",
    "    def _load():\n",
    "        \"\"\" Creating relative path and then loading the df_path \"\"\"\n",
    "        \"\"\"\n",
    "        df_path = os.path.join(os.path.dirname(os.path.abspath(__file__)) +\n",
    "                               os.path.normpath(f'/dfs/{cfg.DATAFRAME_NAME}'))\n",
    "        \"\"\"\n",
    "        df_path = './dfs/{}'.format(DATAFRAME_NAME)\n",
    "        df = pd.read_csv(\n",
    "            df_path,\n",
    "            dtype={\n",
    "                'datetime'\n",
    "                'open': np.float32,\n",
    "                'close': np.float32,\n",
    "                'high': np.float32,\n",
    "                'low': np.float32,\n",
    "                'tickqty': np.float32,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # df['hours']= pd.to_datetime(df['datetime'], format='%Y%m%d %H:%M:%S.%f').dt.hour / 24\n",
    "        df['hours'] = pd.to_datetime(df['date'], format='%m-%d-%Y %H:%M:%S').dt.hour / 24\n",
    "        df['minutes'] = pd.to_datetime(df['date'], format='%m-%d-%Y %H:%M:%S').dt.minute / 64\n",
    "        df['tickqty'] = df['tickqty'] / TICQTY_MAX\n",
    "        return df\n",
    "\n",
    "    def _scale(self, array: np.ndarray, start: int, end: int):\n",
    "        columns = array.T[start: end].T\n",
    "\n",
    "        self.__scaler.fit(columns)\n",
    "        scaled_cols = self.__scaler.transform(columns).T\n",
    "        array.T[start:end] = scaled_cols\n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, batch_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=20000)\n",
    "        self.sample_memory = deque()\n",
    "        \n",
    "        self.gamma = 0.99  # discount rate\n",
    "        \n",
    "        self.epsilon = 0.0 #0.44 # exploration rate\n",
    "        self.epsilon_min = 0.0\n",
    "        self.epsilon_decay = 0.9999\n",
    "        \n",
    "        self.learning_rate = 0.001\n",
    "        self.learning_rate_decay = 0.99999\n",
    "        self.learning_rate_min = 0.0001\n",
    "        \n",
    "        self.batch_size_samples = 500\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    \"\"\"Huber loss for Q Learning\n",
    "\n",
    "    References: https://en.wikipedia.org/wiki/Huber_loss\n",
    "                https://www.tensorflow.org/api_docs/python/tf/losses/huber_loss\n",
    "    \"\"\"\n",
    "\n",
    "    def _huber_loss(self, y_true, y_pred, clip_delta=1.0):\n",
    "        error = y_true - y_pred\n",
    "        cond = K.abs(error) <= clip_delta\n",
    "\n",
    "        squared_loss = 0.5 * K.square(error)\n",
    "        quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
    "\n",
    "        return K.mean(tf.where(cond, squared_loss, quadratic_loss))\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Conv1D(filters=32,\n",
    "                         kernel_size=12,\n",
    "                         activation=\"relu\",\n",
    "                         padding=\"same\",\n",
    "                         input_shape=self.state_size))\n",
    "        \"\"\"\n",
    "        model.add(Conv1D(filters=24,\n",
    "                         kernel_size=8,\n",
    "                         activation=\"relu\",\n",
    "                         padding=\"same\"))\n",
    "                  \n",
    "        \"\"\"         \n",
    "        model.add(Flatten())          \n",
    "        model.add(Dense(units=128, init=\"uniform\", activation=\"relu\"))\n",
    "        model.add(Dense(units=128, init=\"uniform\", activation=\"relu\"))\n",
    "                  \n",
    "        \"\"\"\n",
    "        \n",
    "        model.add(CuDNNLSTM(units=171, return_sequences=True, input_shape=self.state_size))\n",
    "        model.add(CuDNNLSTM(units=114, return_sequences=True)) \n",
    "        model.add(CuDNNLSTM(units=76, return_sequences=True)) \n",
    "        model.add(CuDNNLSTM(units=50, return_sequences=True)) \n",
    "        model.add(CuDNNLSTM(units=33, return_sequences=True))  \n",
    "        model.add(CuDNNLSTM(units=22, return_sequences=True))  \n",
    "        model.add(CuDNNLSTM(units=15, return_sequences=True))\n",
    "        model.add(CuDNNLSTM(units=10, return_sequences=True))\n",
    "        model.add(CuDNNLSTM(units=7, return_sequences=True))\n",
    "        model.add(CuDNNLSTM(units=5, return_sequences=True))\n",
    "        model.add(CuDNNLSTM(units=3, return_sequences=False))\n",
    "        \"\"\"\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss=self._huber_loss,\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        self.sample_memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train_from_iterations(self):\n",
    "        while True:\n",
    "            if len(self.sample_memory) != 0:\n",
    "                state, action, reward, next_state, done = self.sample_memory.popleft()\n",
    "                self.train(state, action, reward, next_state, done)\n",
    "\n",
    "    def train(self, state, action, reward, next_state, done):\n",
    "        target = self.model.predict(state, steps=1, verbose=0)\n",
    "        if done and reward > 80 * TIMES_FACTOR:\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            # a = self.model.predict(next_state)[0]\n",
    "            t = self.target_model.predict(next_state)[0]\n",
    "            target[0][action] = reward + self.gamma * np.amax(t)\n",
    "            # target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
    "        self.model.fit(state, target, epochs=1, verbose=0)\n",
    "        # print('done')\n",
    "\n",
    "    def act(self, state):\n",
    "        if not isinstance(state, np.ndarray):\n",
    "            return 0\n",
    "\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size), True\n",
    "        act_values = self.model.predict(state, steps=1)\n",
    "        return np.argmax(act_values[0]), False  # returns action\n",
    "\n",
    "    def predict(self, state):\n",
    "        act_values = self.model.predict(state, steps=1)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self):\n",
    "        while True:\n",
    "            minibatch = random.sample(self.memory, self.batch_size)\n",
    "            for state, action, reward, next_state, done in minibatch:\n",
    "                if not isinstance(state, np.ndarray):\n",
    "                    continue\n",
    "                \n",
    "                self.train(state, action, reward, next_state, done)\n",
    "                \"\"\"\n",
    "                target = self.model.predict(state, steps=1, verbose=0)\n",
    "                if done and reward > 80 * TIMES_FACTOR:\n",
    "                    target[0][action] = reward\n",
    "                else:\n",
    "                    # a = self.model.predict(next_state)[0]\n",
    "                    t = self.target_model.predict(next_state)[0]\n",
    "                    target[0][action] = reward + self.gamma * np.amax(t)\n",
    "                    # target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
    "                self.model.fit(state, target, epochs=1, verbose=0)\n",
    "                \"\"\"\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "            else:\n",
    "                self.epsilon = self.epsilon_min\n",
    "            \n",
    "            if self.learning_rate > self.learning_rate_min:\n",
    "                self.learning_rate *= self.learning_rate_decay\n",
    "            else:\n",
    "                self.learning_rate = self.learning_rate_min\n",
    "            # print('done')\n",
    "    \n",
    "    def set_learning_rate(self):\n",
    "        K.set_value(self.model.optimizer.lr, self.learning_rate)  # set new lr\n",
    "        K.set_value(self.target_model.optimizer.lr, self.learning_rate)  # set new lr\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "        self.model._make_predict_function()\n",
    "        self.model._make_test_function()\n",
    "        self.model._make_train_function()\n",
    "\n",
    "        self.target_model.load_weights(name)\n",
    "        self.target_model._make_predict_function()\n",
    "        self.target_model._make_test_function()\n",
    "        self.target_model._make_train_function()\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class Trevor:\n",
    "    POSITIVE_TIMES_REWARD = 0 #0.00001\n",
    "    NEGATIVE_TIMES_REWARD = 0 #0.00001\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "        self.cursor = 0\n",
    "        self.enter_price = 0\n",
    "        self.local_max_price = 0\n",
    "\n",
    "        self.last_action = 0\n",
    "\n",
    "        self.closed_counter = 0\n",
    "        self.total_reward = 0\n",
    "        self.trade_counter = 0\n",
    "\n",
    "        self.closed_counter_list = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.cursor = 0\n",
    "        self.enter_price = 0\n",
    "        self.last_action = 0\n",
    "        self.closed_counter = 0\n",
    "        self.trade_counter = 0\n",
    "        self.total_reward = 0\n",
    "        # self.reset_closed_list()\n",
    "\n",
    "        return self.step(0)[0]\n",
    "\n",
    "    def step(self, action):\n",
    "        sample, last_open, last_close = self.df.get(self.cursor)\n",
    "\n",
    "        reward, closing_trade = self.__process_action(action=action, last_close=last_close)\n",
    "        sample = self.__append_last_action(sample=sample, action=action, last_close=last_close)\n",
    "\n",
    "        self.__increment_cursor()\n",
    "\n",
    "        return sample, reward, closing_trade, ''\n",
    "\n",
    "    def get_total_reward(self):\n",
    "        return self.total_reward\n",
    "\n",
    "    def reset_closed_list(self):\n",
    "        self.closed_counter_list = []\n",
    "\n",
    "    def plot(self, title):\n",
    "        x = list(range(1, len(self.closed_counter_list) + 1))\n",
    "        pyplot.plot(x, self.closed_counter_list)\n",
    "        pyplot.title(str(title))\n",
    "        pyplot.show()\n",
    "\n",
    "    def __process_action(self, action, last_close):\n",
    "        if action < 0 or action > 2:\n",
    "            raise ValueError('Action have to be inrage (0 - 2) got {action}')\n",
    "\n",
    "        closing_trade = False\n",
    "\n",
    "        # \"\"\" CLOSING POSITION \"\"\"\n",
    "        if (self.last_action == 2 and action == 0) or (self.last_action == 1 and action == 0):\n",
    "            reward = self.__close_trade(last_close=last_close)\n",
    "            closing_trade = True\n",
    "\n",
    "        # \"\"\" CLOSING POSITION AND GOING TO DIFFERENT POSITION \"\"\"\n",
    "        elif (self.last_action == 2 and action == 1) or (self.last_action == 1 and action == 2):\n",
    "            reward = self.__close_trade(last_close=last_close) - CLOSING_TRADE_WITH_OPENING\n",
    "            self.enter_price = last_close\n",
    "            self.local_max_price = last_close\n",
    "            closing_trade = True\n",
    "\n",
    "        # \"\"\" HOLDING OPENED POSITION  \"\"\"\n",
    "        elif (self.last_action == 2 and action == 2) or (self.last_action == 1 and action == 1):\n",
    "            if self.last_action == 2:\n",
    "                if self.local_max_price < last_close:\n",
    "                    reward = (last_close - self.enter_price) * REWARD_FOR_PIPS\n",
    "                    self.local_max_price = last_close\n",
    "\n",
    "                else:\n",
    "                    reward = (last_close - self.local_max_price) * REWARD_FOR_PIPS\n",
    "                    reward = reward / DIVIDE_PRICE_UNDER_LOCAL_MINIMA if last_close > self.enter_price \\\n",
    "                        else reward\n",
    "\n",
    "            else:\n",
    "                if self.local_max_price > last_close:\n",
    "                    reward = (self.enter_price - last_close) * REWARD_FOR_PIPS\n",
    "                    self.local_max_price = last_close\n",
    "\n",
    "                else:\n",
    "                    reward = (self.local_max_price - last_close) * REWARD_FOR_PIPS\n",
    "                    reward = reward / DIVIDE_PRICE_UNDER_LOCAL_MINIMA if last_close < self.enter_price \\\n",
    "                        else reward\n",
    "\n",
    "        # \"\"\" OPENING POSITION  \"\"\"\n",
    "        elif (self.last_action == 0 and action == 1) or (self.last_action == 0 and action == 2):\n",
    "            self.enter_price = last_close\n",
    "            self.local_max_price = last_close\n",
    "            reward = OPEN_TRADE_REWARD\n",
    "\n",
    "        # \"\"\" HOLD \"\"\"\n",
    "        elif self.last_action == 0 and action == 0:\n",
    "            reward = HOLD_REWARD\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Last action = {self.last_action} and actual_action = {action}')\n",
    "\n",
    "        self.last_action = action\n",
    "        self.total_reward += reward\n",
    "        return reward, closing_trade\n",
    "\n",
    "    def __increment_cursor(self):\n",
    "        \"\"\" Incrementing the cursor, if the cursor is bigger than lenght of the dataframe, then reset it\"\"\"\n",
    "\n",
    "        self.cursor += 1\n",
    "        if self.cursor > self.df.lenght:\n",
    "            self.cursor = 0\n",
    "\n",
    "    def __close_trade(self, last_close):\n",
    "        if self.last_action == 2:\n",
    "            reward = (last_close - self.enter_price) * REWARD_FOR_PIPS \n",
    "            if reward < 0:\n",
    "                reward = reward * (TIMES_FACTOR/NEGATIVE_REWARD_DIVIDE)\n",
    "                self.closed_counter += reward / (TIMES_FACTOR/NEGATIVE_REWARD_DIVIDE)\n",
    "            if reward > 0:\n",
    "                reward = reward * TIMES_FACTOR\n",
    "                self.closed_counter += reward / TIMES_FACTOR\n",
    "            \n",
    "            \n",
    "            reward += self.POSITIVE_TIMES_REWARD * pow(reward, 3) if reward > 0 \\\n",
    "                else self.NEGATIVE_TIMES_REWARD * pow(reward, 3)\n",
    "\n",
    "        else:\n",
    "            reward = (self.enter_price - last_close) * REWARD_FOR_PIPS \n",
    "            if reward < 0:\n",
    "                reward = reward * (TIMES_FACTOR/NEGATIVE_REWARD_DIVIDE)\n",
    "                self.closed_counter += reward / (TIMES_FACTOR/NEGATIVE_REWARD_DIVIDE)\n",
    "            if reward > 0:\n",
    "                reward = reward * TIMES_FACTOR\n",
    "                self.closed_counter += reward / TIMES_FACTOR\n",
    "                \n",
    "            \n",
    "            reward += self.POSITIVE_TIMES_REWARD * pow(reward, 3) if reward > 0 \\\n",
    "                else self.NEGATIVE_TIMES_REWARD * pow(reward, 3)\n",
    "\n",
    "        self.closed_counter_list.append(self.closed_counter)\n",
    "        self.trade_counter += 1\n",
    "        return reward\n",
    "\n",
    "    def __append_last_action(self, sample: np.ndarray, action: int, last_close: float):\n",
    "        how_many = sample.shape[1]\n",
    "        decoded_action = ACTION_DECODE[action]\n",
    "\n",
    "        action_arr = (np.expand_dims(np.asarray([decoded_action for i in range(0, how_many)]), axis=1))\n",
    "\n",
    "        if action == 2 or action == 1:\n",
    "            dif = (last_close - self.enter_price)\n",
    "            pip_difference = (np.expand_dims(np.asarray([dif for i in range(0, how_many)]), axis=1))\n",
    "\n",
    "        else:\n",
    "            dif = 0\n",
    "            pip_difference = (np.expand_dims(np.asarray([dif for i in range(0, how_many)]), axis=1))\n",
    "\n",
    "        sample = np.append(sample[0], action_arr, axis=1)\n",
    "        sample = np.append(sample, pip_difference, axis=1)\n",
    "\n",
    "        return np.expand_dims(sample, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lukas\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:57: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=128, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "C:\\Users\\Lukas\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:58: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=128, activation=\"relu\", kernel_initializer=\"uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than 20 on candle 233\n",
      "More than 20 on candle 431\n",
      "More than 20 on candle 516\n",
      "More than 20 on candle 566\n",
      "More than 20 on candle 657\n",
      "More than 20 on candle 675\n",
      "More than 20 on candle 760\n",
      "More than 20 on candle 852\n",
      "More than 20 on candle 889\n",
      "More than 20 on candle 906\n",
      "More than 20 on candle 911\n",
      "More than 20 on candle 958\n",
      "More than 20 on candle 979\n",
      "More than 20 on candle 995\n",
      "More than 20 on candle 1001\n",
      "More than 20 on candle 1086\n",
      "More than 20 on candle 1114\n",
      "More than 20 on candle 1176\n",
      "More than 20 on candle 1199\n",
      "More than 20 on candle 1249\n",
      "More than 20 on candle 1267\n",
      "More than 20 on candle 1337\n",
      "More than 20 on candle 1353\n",
      "More than 20 on candle 1487\n",
      "More than 20 on candle 1661\n",
      "More than 20 on candle 1687\n",
      "More than 20 on candle 1768\n",
      "More than 20 on candle 1798\n",
      "More than 20 on candle 1824\n",
      "More than 20 on candle 1883\n",
      "More than 20 on candle 1933\n",
      "More than 20 on candle 1973\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting to train the whole dataset\n",
      "action0:  622 \n",
      "action1:  715 \n",
      "action2:  643\n",
      "DONE, lets roll!!\n",
      "165.807\n",
      "More than 20 on candle 29\n",
      "More than 20 on candle 79\n",
      "More than 20 on candle 114\n",
      "More than 20 on candle 119\n",
      "More than 20 on candle 170\n",
      "More than 20 on candle 203\n",
      "More than 20 on candle 233\n",
      "More than 20 on candle 275\n",
      "More than 20 on candle 311\n",
      "More than 20 on candle 365\n",
      "More than 20 on candle 385\n",
      "More than 20 on candle 431\n",
      "More than 20 on candle 492\n",
      "More than 20 on candle 509\n",
      "More than 20 on candle 515\n",
      "More than 20 on candle 565\n",
      "More than 20 on candle 582\n",
      "More than 20 on candle 653\n"
     ]
    }
   ],
   "source": [
    "env = Trevor(Dataframe())\n",
    "state_size = (NUMBER_OF_SAMPLES, 9)\n",
    "action_size = 3\n",
    "batch_size = 128 #32\n",
    "agent = DQNAgent(state_size, action_size, batch_size)\n",
    "\n",
    "#agent.save(\"./save/cartpole-ddqn.h5\")\n",
    "agent.load(\"./save/cartpole-ddqn.h5\")\n",
    "\n",
    "closed = False\n",
    "run = False\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    strt = t_lib.time()\n",
    "    \n",
    "    akce0 = 0\n",
    "    akce1 = 0\n",
    "    akce2 = 0\n",
    "    \n",
    "    for time in range(env.df.lenght):\n",
    "        action, random_action = agent.act(state)\n",
    "\n",
    "        if action > 3 or action < 0:\n",
    "            print('Got action ' + action)\n",
    "            continue\n",
    "\n",
    "        next_state, reward, closed, _ = env.step(action)\n",
    "\n",
    "        if not isinstance(next_state, np.ndarray) or not(state, np.ndarray):\n",
    "            print(next_state)\n",
    "            print('NOT NUMPY!!')\n",
    "            continue\n",
    "\n",
    "        agent.memorize(state=state, action=action, reward=reward, next_state=next_state, done=closed)\n",
    "        state = next_state\n",
    "        \n",
    "        \"\"\"\n",
    "        print(f'Actual reward = {round(reward, 1)},\\t total reward = {round(env.total_reward, 1)},'\n",
    "              f'\\t action = {action}, \\t trade_counter = {round(env.trade_counter, 1)}, '\n",
    "              f'\\t pip_counter = {round(env.closed_counter, 1)}'\n",
    "              f'\\t random_action = {random_action}'\n",
    "              f'\\t candle_number = {time}')\n",
    "        \"\"\"\n",
    "        if action == 0:\n",
    "            akce0 += 1\n",
    "        if action == 1:\n",
    "            akce1 += 1\n",
    "        if action == 2:\n",
    "            akce2 += 1\n",
    "        \n",
    "        # print(\"Actual reward = {}\\t, total reward = {},\\t action = {}\\t trade_counter = {}\\t pip_counter = {}\".format(round(reward, 1), round(env.total_reward, 1), action, round(env.trade_counter, 1), round(env.closed_counter, 1)))\n",
    "        if closed and reward > 20 * TIMES_FACTOR:\n",
    "            agent.update_target_model()\n",
    "            \"\"\"\n",
    "            print(\"episode: {}/{}, score: {}, e: {}, lr: {}\"\n",
    "                  .format(e, EPISODES, time, round(agent.epsilon, 2)), round(agent.learning_rate, 2))\n",
    "            \"\"\"\n",
    "            print('More than 20 on candle {}'.format(time))\n",
    "        \n",
    "        if len(agent.memory) > batch_size:\n",
    "            # agent.replay(batch_size)\n",
    "            if not run:\n",
    "                thr_list = [Thread(target=agent.replay) for _ in range(1)]\n",
    "                for thr in thr_list:\n",
    "                    thr.start()\n",
    "                    t_lib.sleep(1)\n",
    "                    \n",
    "                thr_list = [Thread(target=agent.train_from_iterations) for _ in range(4)]\n",
    "                for thr in thr_list:\n",
    "                    thr.start()\n",
    "                    t_lib.sleep(1)\n",
    "                \n",
    "                run = True\n",
    "                \n",
    "    # clear_output()\n",
    "    env.plot(title='total reward ={};  e = {}, lr={}, episode = {}'.format(round(env.total_reward, 2), round(agent.epsilon, 4), round(agent.learning_rate, 7), e))\n",
    "    env.reset_closed_list()\n",
    "    print('Waiting to train the whole dataset')\n",
    "    print(\"action0: \", akce0,\"\\naction1: \", akce1,\"\\naction2: \", akce2)\n",
    "    while not len(agent.sample_memory) == 0:\n",
    "        pass\n",
    "    agent.set_learning_rate()\n",
    "    print('DONE, lets roll!!')\n",
    "    agent.save(\"./save/cartpole-ddqn.h5\")\n",
    "    print(round(t_lib.time() - strt, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
