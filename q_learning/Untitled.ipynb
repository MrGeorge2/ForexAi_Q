{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, CuDNNLSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import time as t_lib\n",
    "import tensorflow as tf\n",
    "from threading import Thread\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFRAME_NAME = 'EURUSD_m15_Ask_ready.csv'\n",
    "NUMBER_OF_SAMPLES = 50\n",
    "\n",
    "EPISODES = 5000\n",
    "TICQTY_MAX = 55000\n",
    "HOLD_REWARD = -1\n",
    "OPEN_TRADE_REWARD = 0\n",
    "CLOSING_TRADE_WITH_OPENING = 5\n",
    "DIVIDE_PRICE_UNDER_LOCAL_MINIMA = 10\n",
    "REWARD_FOR_PIPS = 10000\n",
    "TIMES_FACTOR = 1\n",
    "\n",
    "ACTION_DECODE = {\n",
    "    0: 0,\n",
    "    1: 0.5,\n",
    "    2: 1,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class Dataframe:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._dataframe = self._load()[:2000]\n",
    "        self.__scaler = MinMaxScaler()\n",
    "\n",
    "    @property\n",
    "    def lenght(self):\n",
    "        return len(self._dataframe.index) - NUMBER_OF_SAMPLES\n",
    "\n",
    "    def get(self, sample_number):\n",
    "        if sample_number > self.lenght or sample_number < 0:\n",
    "            raise ValueError(\"Sample number out of range (0 - {self.lenght})\")\n",
    "\n",
    "        start_index = sample_number\n",
    "        end_index = start_index + NUMBER_OF_SAMPLES\n",
    "\n",
    "        df_sample = self._dataframe[start_index: end_index]\n",
    "\n",
    "        last_open = df_sample.at[df_sample.index[-1], 'open']\n",
    "        last_close = df_sample.at[df_sample.index[-1], 'close']\n",
    "\n",
    "        df_sample = df_sample[['open', 'close', 'high', 'low', 'tickqty', 'hours', 'minutes']].values\n",
    "        df_sample = self._scale(df_sample, start=0, end=4)\n",
    "        return np.expand_dims(df_sample, axis=0), last_open, last_close\n",
    "\n",
    "    @staticmethod\n",
    "    def _load():\n",
    "        \"\"\" Creating relative path and then loading the df_path \"\"\"\n",
    "        \"\"\"\n",
    "        df_path = os.path.join(os.path.dirname(os.path.abspath(__file__)) +\n",
    "                               os.path.normpath(f'/dfs/{cfg.DATAFRAME_NAME}'))\n",
    "        \"\"\"\n",
    "        df_path = './dfs/{}'.format(DATAFRAME_NAME)\n",
    "        df = pd.read_csv(\n",
    "            df_path,\n",
    "            dtype={\n",
    "                'datetime'\n",
    "                'open': np.float32,\n",
    "                'close': np.float32,\n",
    "                'high': np.float32,\n",
    "                'low': np.float32,\n",
    "                'tickqty': np.float32,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # df['hours']= pd.to_datetime(df['datetime'], format='%Y%m%d %H:%M:%S.%f').dt.hour / 24\n",
    "        df['hours'] = pd.to_datetime(df['date'], format='%m-%d-%Y %H:%M:%S').dt.hour / 24\n",
    "        df['minutes'] = pd.to_datetime(df['date'], format='%m-%d-%Y %H:%M:%S').dt.minute / 64\n",
    "        df['tickqty'] = df['tickqty'] / TICQTY_MAX\n",
    "        return df\n",
    "\n",
    "    def _scale(self, array: np.ndarray, start: int, end: int):\n",
    "        columns = array.T[start: end].T\n",
    "\n",
    "        self.__scaler.fit(columns)\n",
    "        scaled_cols = self.__scaler.transform(columns).T\n",
    "        array.T[start:end] = scaled_cols\n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, batch_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=20000)\n",
    "        self.sample_memory = deque()\n",
    "        \n",
    "        self.gamma = 0.99  # discount rate\n",
    "        \n",
    "        self.epsilon = 0.001 # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.99999\n",
    "        \n",
    "        self.learning_rate = 0.002\n",
    "        self.learning_rate_decay = 0.9992\n",
    "        self.learning_rate_min = 0.002\n",
    "        \n",
    "        self.batch_size_samples = 500\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    \"\"\"Huber loss for Q Learning\n",
    "\n",
    "    References: https://en.wikipedia.org/wiki/Huber_loss\n",
    "                https://www.tensorflow.org/api_docs/python/tf/losses/huber_loss\n",
    "    \"\"\"\n",
    "\n",
    "    def _huber_loss(self, y_true, y_pred, clip_delta=1.0):\n",
    "        error = y_true - y_pred\n",
    "        cond = K.abs(error) <= clip_delta\n",
    "\n",
    "        squared_loss = 0.5 * K.square(error)\n",
    "        quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
    "\n",
    "        return K.mean(tf.where(cond, squared_loss, quadratic_loss))\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(CuDNNLSTM(units=50, return_sequences=True, input_shape=self.state_size))\n",
    "\n",
    "        model.add(CuDNNLSTM(units=33, return_sequences=False))\n",
    "        \n",
    "        model.add(Dense(units=22, activation='relu'))\n",
    "\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss=self._huber_loss,\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        self.sample_memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train_from_iterations(self):\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            samples = []\n",
    "            for _ in range(self.batch_size_samples):\n",
    "                if len(self.sample_memory) != 0:\n",
    "                    samples.append(self.sample_memory.popleft())\n",
    "                else:\n",
    "                    break\n",
    "            if len(samples) != 0:\n",
    "                for state, action, reward, next_state, done in samples:\n",
    "                    self.train(state, action, reward, next_state, done)   \n",
    "        \"\"\"\n",
    "        while True:\n",
    "            if len(self.sample_memory) != 0:\n",
    "                state, action, reward, next_state, done = self.sample_memory.popleft()\n",
    "                self.train(state, action, reward, next_state, done)\n",
    "\n",
    "    def train(self, state, action, reward, next_state, done):\n",
    "        target = self.model.predict(state, steps=1, verbose=0)\n",
    "        if done and reward > 80 * TIMES_FACTOR:\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            # a = self.model.predict(next_state)[0]\n",
    "            t = self.target_model.predict(next_state)[0]\n",
    "            target[0][action] = reward + self.gamma * np.amax(t)\n",
    "            # target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
    "        self.model.fit(state, target, epochs=1, verbose=0)\n",
    "        # print('done')\n",
    "\n",
    "    def act(self, state):\n",
    "        if not isinstance(state, np.ndarray):\n",
    "            return 0\n",
    "\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size), True\n",
    "        act_values = self.model.predict(state, steps=1)\n",
    "        return np.argmax(act_values[0]), False  # returns action\n",
    "\n",
    "    def predict(self, state):\n",
    "        act_values = self.model.predict(state, steps=1)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self):\n",
    "        while True:\n",
    "            minibatch = random.sample(self.memory, self.batch_size)\n",
    "            for state, action, reward, next_state, done in minibatch:\n",
    "                if not isinstance(state, np.ndarray):\n",
    "                    continue\n",
    "                \n",
    "                self.train(state, action, reward, next_state, done)\n",
    "                \"\"\"\n",
    "                target = self.model.predict(state, steps=1, verbose=0)\n",
    "                if done and reward > 80 * TIMES_FACTOR:\n",
    "                    target[0][action] = reward\n",
    "                else:\n",
    "                    # a = self.model.predict(next_state)[0]\n",
    "                    t = self.target_model.predict(next_state)[0]\n",
    "                    target[0][action] = reward + self.gamma * np.amax(t)\n",
    "                    # target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
    "                self.model.fit(state, target, epochs=1, verbose=0)\n",
    "                \"\"\"\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "            else:\n",
    "                self.epsilon = self.epsilon_min\n",
    "            \n",
    "            if self.learning_rate > self.learning_rate_min:\n",
    "                self.learning_rate *= self.learning_rate_decay\n",
    "            else:\n",
    "                self.learning_rate = self.learning_rate_min\n",
    "            # print('done')\n",
    "    \n",
    "    def set_learning_rate(self):\n",
    "        K.set_value(self.model.optimizer.lr, self.learning_rate)  # set new lr\n",
    "        K.set_value(self.target_model.optimizer.lr, self.learning_rate)  # set new lr\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "        self.model._make_predict_function()\n",
    "        self.model._make_test_function()\n",
    "        self.model._make_train_function()\n",
    "\n",
    "        self.target_model.load_weights(name)\n",
    "        self.target_model._make_predict_function()\n",
    "        self.target_model._make_test_function()\n",
    "        self.target_model._make_train_function()\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trevor:\n",
    "    POSITIVE_TIMES_REWARD = 0.00001\n",
    "    NEGATIVE_TIMES_REWARD = 0.00001\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "        self.cursor = 0\n",
    "        self.enter_price = 0\n",
    "        self.local_max_price = 0\n",
    "\n",
    "        self.last_action = 0\n",
    "\n",
    "        self.closed_counter = 0\n",
    "        self.total_reward = 0\n",
    "        self.trade_counter = 0\n",
    "\n",
    "        self.closed_counter_list = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.cursor = 0\n",
    "        self.enter_price = 0\n",
    "        self.last_action = 0\n",
    "        self.closed_counter = 0\n",
    "        self.trade_counter = 0\n",
    "        self.total_reward = 0\n",
    "        # self.reset_closed_list()\n",
    "\n",
    "        return self.step(0)[0]\n",
    "\n",
    "    def step(self, action):\n",
    "        sample, last_open, last_close = self.df.get(self.cursor)\n",
    "\n",
    "        reward, closing_trade = self.__process_action(action=action, last_close=last_close)\n",
    "        sample = self.__append_last_action(sample=sample, action=action, last_close=last_close)\n",
    "\n",
    "        self.__increment_cursor()\n",
    "\n",
    "        return sample, reward, closing_trade, ''\n",
    "\n",
    "    def get_total_reward(self):\n",
    "        return self.total_reward\n",
    "\n",
    "    def reset_closed_list(self):\n",
    "        self.closed_counter_list = []\n",
    "\n",
    "    def plot(self, title):\n",
    "        x = list(range(1, len(self.closed_counter_list) + 1))\n",
    "        pyplot.plot(x, self.closed_counter_list)\n",
    "        pyplot.title(str(title))\n",
    "        pyplot.show()\n",
    "\n",
    "    def __process_action(self, action, last_close):\n",
    "        if action < 0 or action > 2:\n",
    "            raise ValueError('Action have to be inrage (0 - 2) got {action}')\n",
    "\n",
    "        closing_trade = False\n",
    "\n",
    "        # \"\"\" CLOSING POSITION \"\"\"\n",
    "        if (self.last_action == 2 and action == 0) or (self.last_action == 1 and action == 0):\n",
    "            reward = self.__close_trade(last_close=last_close)\n",
    "            closing_trade = True\n",
    "\n",
    "        # \"\"\" CLOSING POSITION AND GOING TO DIFFERENT POSITION \"\"\"\n",
    "        elif (self.last_action == 2 and action == 1) or (self.last_action == 1 and action == 2):\n",
    "            reward = self.__close_trade(last_close=last_close) - CLOSING_TRADE_WITH_OPENING\n",
    "            self.enter_price = last_close\n",
    "            self.local_max_price = last_close\n",
    "            closing_trade = True\n",
    "\n",
    "        # \"\"\" HOLDING OPENED POSITION  \"\"\"\n",
    "        elif (self.last_action == 2 and action == 2) or (self.last_action == 1 and action == 1):\n",
    "            if self.last_action == 2:\n",
    "                if self.local_max_price < last_close:\n",
    "                    reward = (last_close - self.enter_price) * REWARD_FOR_PIPS\n",
    "                    self.local_max_price = last_close\n",
    "\n",
    "                else:\n",
    "                    reward = (last_close - self.local_max_price) * REWARD_FOR_PIPS\n",
    "                    reward = reward / DIVIDE_PRICE_UNDER_LOCAL_MINIMA if last_close > self.enter_price \\\n",
    "                        else reward\n",
    "\n",
    "            else:\n",
    "                if self.local_max_price > last_close:\n",
    "                    reward = (self.enter_price - last_close) * REWARD_FOR_PIPS\n",
    "                    self.local_max_price = last_close\n",
    "\n",
    "                else:\n",
    "                    reward = (self.local_max_price - last_close) * REWARD_FOR_PIPS\n",
    "                    reward = reward / DIVIDE_PRICE_UNDER_LOCAL_MINIMA if last_close < self.enter_price \\\n",
    "                        else reward\n",
    "            reward = OPEN_TRADE_REWARD\n",
    "\n",
    "        # \"\"\" OPENING POSITION  \"\"\"\n",
    "        elif (self.last_action == 0 and action == 1) or (self.last_action == 0 and action == 2):\n",
    "            self.enter_price = last_close\n",
    "            self.local_max_price = last_close\n",
    "            # reward = HOLD_REWARD\n",
    "            reward = OPEN_TRADE_REWARD\n",
    "\n",
    "        # \"\"\" HOLD \"\"\"\n",
    "        elif self.last_action == 0 and action == 0:\n",
    "            reward = HOLD_REWARD\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Last action = {self.last_action} and actual_action = {action}')\n",
    "\n",
    "        self.last_action = action\n",
    "        self.total_reward += reward\n",
    "        return reward, closing_trade\n",
    "\n",
    "    def __increment_cursor(self):\n",
    "        \"\"\" Incrementing the cursor, if the cursor is bigger than lenght of the dataframe, then reset it\"\"\"\n",
    "\n",
    "        self.cursor += 1\n",
    "        if self.cursor > self.df.lenght:\n",
    "            self.cursor = 0\n",
    "\n",
    "    def __close_trade(self, last_close):\n",
    "        if self.last_action == 2:\n",
    "            reward = (last_close - self.enter_price) * REWARD_FOR_PIPS * TIMES_FACTOR\n",
    "            self.closed_counter += reward / TIMES_FACTOR\n",
    "            reward += self.POSITIVE_TIMES_REWARD * pow(reward, 3) if reward > 0 \\\n",
    "                else self.NEGATIVE_TIMES_REWARD * pow(reward, 3)\n",
    "\n",
    "        else:\n",
    "            reward = (self.enter_price - last_close) * REWARD_FOR_PIPS * TIMES_FACTOR\n",
    "            self.closed_counter += reward / TIMES_FACTOR\n",
    "            reward += self.POSITIVE_TIMES_REWARD * pow(reward, 3) if reward > 0 \\\n",
    "                else self.NEGATIVE_TIMES_REWARD * pow(reward, 3)\n",
    "\n",
    "        self.closed_counter_list.append(self.closed_counter)\n",
    "        self.trade_counter += 1\n",
    "        return reward\n",
    "\n",
    "    def __append_last_action(self, sample: np.ndarray, action: int, last_close: float):\n",
    "        how_many = sample.shape[1]\n",
    "        decoded_action = ACTION_DECODE[action]\n",
    "\n",
    "        action_arr = (np.expand_dims(np.asarray([decoded_action for i in range(0, how_many)]), axis=1))\n",
    "\n",
    "        if action == 2 or action == 1:\n",
    "            dif = (last_close - self.enter_price)\n",
    "            pip_difference = (np.expand_dims(np.asarray([dif for i in range(0, how_many)]), axis=1))\n",
    "\n",
    "        else:\n",
    "            dif = 0\n",
    "            pip_difference = (np.expand_dims(np.asarray([dif for i in range(0, how_many)]), axis=1))\n",
    "\n",
    "        sample = np.append(sample[0], action_arr, axis=1)\n",
    "        sample = np.append(sample, pip_difference, axis=1)\n",
    "\n",
    "        return np.expand_dims(sample, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hejdu\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\hejdu\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting to train the whole dataset\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-08573ad0a9c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_closed_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Waiting to train the whole dataset'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_memory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_learning_rate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = Trevor(Dataframe())\n",
    "state_size = (NUMBER_OF_SAMPLES, 9)\n",
    "action_size = 3\n",
    "batch_size = 32\n",
    "agent = DQNAgent(state_size, action_size, batch_size)\n",
    "\n",
    "# agent.save(\"./save/cartpole-ddqn.h5\")\n",
    "agent.load(\"./save/cartpole-ddqn.h5\")\n",
    "\n",
    "closed = False\n",
    "run = False\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    strt = t_lib.time()\n",
    "    \n",
    "    for time in range(env.df.lenght):\n",
    "        action, random_action = agent.act(state)\n",
    "\n",
    "        if action > 3 or action < 0:\n",
    "            print('Got action ' + action)\n",
    "            continue\n",
    "\n",
    "        next_state, reward, closed, _ = env.step(action)\n",
    "\n",
    "        if not isinstance(next_state, np.ndarray) or not(state, np.ndarray):\n",
    "            print(next_state)\n",
    "            print('NOT NUMPY!!')\n",
    "            continue\n",
    "\n",
    "        agent.memorize(state=state, action=action, reward=reward, next_state=next_state, done=closed)\n",
    "        state = next_state\n",
    "        \n",
    "        \"\"\"\n",
    "        print(f'Actual reward = {round(reward, 1)},\\t total reward = {round(env.total_reward, 1)},'\n",
    "              f'\\t action = {action}, \\t trade_counter = {round(env.trade_counter, 1)}, '\n",
    "              f'\\t pip_counter = {round(env.closed_counter, 1)}'\n",
    "              f'\\t random_action = {random_action}'\n",
    "              f'\\t candle_number = {time}')\n",
    "        \"\"\"\n",
    "        # print(\"Actual reward = {}\\t, total reward = {},\\t action = {}\\t trade_counter = {}\\t pip_counter = {}\".format(round(reward, 1), round(env.total_reward, 1), action, round(env.trade_counter, 1), round(env.closed_counter, 1)))\n",
    "        if closed and reward > 80 * TIMES_FACTOR:\n",
    "            agent.update_target_model()\n",
    "            \"\"\"\n",
    "            print(\"episode: {}/{}, score: {}, e: {}, lr: {}\"\n",
    "                  .format(e, EPISODES, time, round(agent.epsilon, 2)), round(agent.learning_rate, 2))\n",
    "            \"\"\"\n",
    "            print('More than 80 on candle {}'.format(time))\n",
    "            \n",
    "        if len(agent.memory) > batch_size:\n",
    "            # agent.replay(batch_size)\n",
    "            if not run:\n",
    "                thr_list = [Thread(target=agent.replay) for _ in range(1)]\n",
    "                for thr in thr_list:\n",
    "                    thr.start()\n",
    "                    t_lib.sleep(1)\n",
    "                    \n",
    "                thr_list = [Thread(target=agent.train_from_iterations) for _ in range(5)]\n",
    "                for thr in thr_list:\n",
    "                    thr.start()\n",
    "                    t_lib.sleep(1)\n",
    "                \n",
    "                run = True\n",
    "                \n",
    "    # clear_output()\n",
    "    env.plot(title='total reward ={};  e = {}, lr={}, episode = {}'.format(round(env.total_reward, 2), round(agent.epsilon, 4), round(agent.learning_rate, 5), e))\n",
    "    env.reset_closed_list()\n",
    "    print('Waiting to train the whole dataset')\n",
    "    while not len(agent.sample_memory) == 0:\n",
    "        pass\n",
    "    agent.set_learning_rate()\n",
    "    print('DONE, lets roll!!')\n",
    "    agent.save(\"./save/cartpole-ddqn.h5\")\n",
    "    round(print(t_lib.time() - strt), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
